{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.image as mpimg\n",
    "import tkinter as tk\n",
    "from matplotlib.figure import Figure\n",
    "from IPython.display import display, HTML\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow.keras as ks\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1510 Records\n",
    "# 752 Records True 0 - 751\n",
    "# 758 Records False 752 - 1509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1510, 1441)\n",
      "(1510, 1440, 2)\n",
      "(1510,)\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "f = r'RRFrame.csv'\n",
    "rrFrame = pd.read_csv(f, encoding='iso-8859-1')\n",
    "f = r'SpO2Frame.csv'\n",
    "spFrame = pd.read_csv(f, encoding='iso-8859-1')\n",
    "f = r'LabelFrame.csv'\n",
    "labelFrame = pd.read_csv(f, encoding='iso-8859-1')\n",
    "print(np.shape(rrFrame))\n",
    "\n",
    "\n",
    "RECORDS = np.shape(rrFrame)[0]\n",
    "TOTAL_POINTS = np.shape(rrFrame)[1] - 1\n",
    "INPUT = np.zeros((RECORDS,TOTAL_POINTS,2), dtype=np.float64)\n",
    "OUTPUT = np.zeros((RECORDS),dtype=np.float64)\n",
    "\n",
    "for i in range(RECORDS):\n",
    "    INPUT[i,:,0] = rrFrame.iloc[i][1:1441]\n",
    "    if(np.isnan(INPUT[i,:,0]).any()):\n",
    "        print('rr '+ str(i))\n",
    "    INPUT[i,:,1] = spFrame.iloc[i][1:1441]\n",
    "    if(np.isnan(INPUT[i,:,1]).any()):\n",
    "            print('sp '+ str(i))\n",
    "    if(int(labelFrame.iloc[i]) == 0):\n",
    "        OUTPUT[i] = 0\n",
    "    if(int(labelFrame.iloc[i]) == 1):\n",
    "        OUTPUT[i] = 1\n",
    "\n",
    "print(np.shape(INPUT))\n",
    "print(np.shape(OUTPUT))\n",
    "# SHIFTING INPUT OF RR AND SPO2 BY SUITABLE VALUES\n",
    "INPUT[:,:,0] = (INPUT[:,:,0] - 25)/10\n",
    "INPUT[:,:,1] = (INPUT[:,:,1] - 93)/10\n",
    "#print(INPUT)\n",
    "print(np.isnan(INPUT).any())\n",
    "print(np.isnan(OUTPUT).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1440, 2)\n",
      "(800,)\n",
      "(200, 1440, 2)\n",
      "(200,)\n",
      "(510, 1440, 2)\n",
      "(510,)\n"
     ]
    }
   ],
   "source": [
    "# Take 400 of both cases for training\n",
    "# Take 100 of both cases for validation\n",
    "# Take remaining of both cases for testing\n",
    "TN = 400\n",
    "VN = 100\n",
    "E1 = 752\n",
    "E2 = 1510\n",
    "\n",
    "\n",
    "X_TRAIN = INPUT[0:TN,:,:]\n",
    "X_TRAIN = np.concatenate((X_TRAIN,INPUT[E1:E1+TN,:,:]),axis=0)\n",
    "Y_TRAIN = OUTPUT[0:TN]\n",
    "Y_TRAIN = np.concatenate((Y_TRAIN,OUTPUT[E1:E1+TN]),axis=0)\n",
    "\n",
    "X_VALIDATE = INPUT[TN:TN+VN,:,:]\n",
    "X_VALIDATE = np.concatenate((X_VALIDATE,INPUT[E1+TN:E1+TN+VN,:,:]),axis=0)\n",
    "Y_VALIDATE = OUTPUT[TN:TN+VN]\n",
    "Y_VALIDATE = np.concatenate((Y_VALIDATE,OUTPUT[E1+TN:E1+TN+VN]),axis=0)\n",
    "\n",
    "X_TEST = INPUT[TN+VN:E1,:,:]\n",
    "X_TEST = np.concatenate((X_TEST,INPUT[E1+TN+VN:E2,:,:]),axis=0)\n",
    "Y_TEST = OUTPUT[TN+VN:E1]\n",
    "Y_TEST = np.concatenate((Y_TEST,OUTPUT[E1+TN+VN:E2]),axis=0)\n",
    "\n",
    "print(np.shape(X_TRAIN))\n",
    "print(np.shape(Y_TRAIN))\n",
    "print(np.shape(X_VALIDATE))\n",
    "print(np.shape(Y_VALIDATE))\n",
    "print(np.shape(X_TEST))\n",
    "print(np.shape(Y_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 30)                3960      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 3,991\n",
      "Trainable params: 3,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "seed(7)\n",
    "model = Sequential()\n",
    "model.add(LSTM(30, input_shape=(X_TRAIN.shape[1], X_TRAIN.shape[2]),return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc','mse'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 6.6432 - acc: 0.3333 - mse: 0.6771    \n",
      "Epoch 00001: val_loss improved from inf to 3.79739, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 3s 4ms/sample - loss: 5.0032 - acc: 0.5000 - mse: 0.5100 - val_loss: 3.7974 - val_acc: 0.5000 - val_mse: 0.4703\n",
      "Epoch 2/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 7.1661 - acc: 0.0000e+00 - mse: 0.9108\n",
      "Epoch 00002: val_loss improved from 3.79739 to 3.31604, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 428us/sample - loss: 3.6532 - acc: 0.5000 - mse: 0.4656 - val_loss: 3.3160 - val_acc: 0.5000 - val_mse: 0.4494\n",
      "Epoch 3/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 5.3697 - acc: 0.0000e+00 - mse: 0.8398\n",
      "Epoch 00003: val_loss improved from 3.31604 to 2.80953, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 331us/sample - loss: 2.7864 - acc: 0.5000 - mse: 0.4386 - val_loss: 2.8095 - val_acc: 0.5000 - val_mse: 0.4328\n",
      "Epoch 4/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 2.8020 - acc: 0.3350 - mse: 0.5370\n",
      "Epoch 00004: val_loss improved from 2.80953 to 2.48055, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 377us/sample - loss: 2.1682 - acc: 0.5013 - mse: 0.4172 - val_loss: 2.4805 - val_acc: 0.5000 - val_mse: 0.4222\n",
      "Epoch 5/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 2.3473 - acc: 0.3350 - mse: 0.5124\n",
      "Epoch 00005: val_loss improved from 2.48055 to 2.32382, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 428us/sample - loss: 1.8374 - acc: 0.5013 - mse: 0.4026 - val_loss: 2.3238 - val_acc: 0.5000 - val_mse: 0.4148\n",
      "Epoch 6/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 2.0443 - acc: 0.3367 - mse: 0.4938\n",
      "Epoch 00006: val_loss improved from 2.32382 to 2.17198, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 367us/sample - loss: 1.6181 - acc: 0.5025 - mse: 0.3918 - val_loss: 2.1720 - val_acc: 0.4950 - val_mse: 0.4087\n",
      "Epoch 7/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.8401 - acc: 0.3483 - mse: 0.4791\n",
      "Epoch 00007: val_loss improved from 2.17198 to 2.09025, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 365us/sample - loss: 1.4710 - acc: 0.5113 - mse: 0.3834 - val_loss: 2.0903 - val_acc: 0.4900 - val_mse: 0.4035\n",
      "Epoch 8/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.7218 - acc: 0.3500 - mse: 0.4675\n",
      "Epoch 00008: val_loss improved from 2.09025 to 1.96227, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 1.3870 - acc: 0.5125 - mse: 0.3767 - val_loss: 1.9623 - val_acc: 0.4750 - val_mse: 0.3990\n",
      "Epoch 9/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.6450 - acc: 0.3517 - mse: 0.4580\n",
      "Epoch 00009: val_loss improved from 1.96227 to 1.88293, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 356us/sample - loss: 1.3329 - acc: 0.5138 - mse: 0.3710 - val_loss: 1.8829 - val_acc: 0.4800 - val_mse: 0.3946\n",
      "Epoch 10/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.5721 - acc: 0.3500 - mse: 0.4501\n",
      "Epoch 00010: val_loss improved from 1.88293 to 1.77183, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 400us/sample - loss: 1.2806 - acc: 0.5125 - mse: 0.3661 - val_loss: 1.7718 - val_acc: 0.4650 - val_mse: 0.3902\n",
      "Epoch 11/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.4684 - acc: 0.3500 - mse: 0.4434\n",
      "Epoch 00011: val_loss improved from 1.77183 to 1.72462, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 1.2044 - acc: 0.5125 - mse: 0.3618 - val_loss: 1.7246 - val_acc: 0.4650 - val_mse: 0.3858\n",
      "Epoch 12/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.4166 - acc: 0.3517 - mse: 0.4375\n",
      "Epoch 00012: val_loss improved from 1.72462 to 1.60322, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 351us/sample - loss: 1.1670 - acc: 0.5138 - mse: 0.3580 - val_loss: 1.6032 - val_acc: 0.4650 - val_mse: 0.3817\n",
      "Epoch 13/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.3207 - acc: 0.3517 - mse: 0.4320\n",
      "Epoch 00013: val_loss improved from 1.60322 to 1.40978, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 383us/sample - loss: 1.0960 - acc: 0.5138 - mse: 0.3543 - val_loss: 1.4098 - val_acc: 0.4600 - val_mse: 0.3773\n",
      "Epoch 14/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.2355 - acc: 0.3517 - mse: 0.4268\n",
      "Epoch 00014: val_loss improved from 1.40978 to 1.16162, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 376us/sample - loss: 1.0327 - acc: 0.5138 - mse: 0.3506 - val_loss: 1.1616 - val_acc: 0.4600 - val_mse: 0.3728\n",
      "Epoch 15/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.5438 - acc: 0.0375 - mse: 0.5721\n",
      "Epoch 00015: val_loss improved from 1.16162 to 1.05596, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 334us/sample - loss: 0.9840 - acc: 0.5138 - mse: 0.3471 - val_loss: 1.0560 - val_acc: 0.4600 - val_mse: 0.3682\n",
      "Epoch 16/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.1248 - acc: 0.3517 - mse: 0.4167\n",
      "Epoch 00016: val_loss improved from 1.05596 to 1.02127, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 361us/sample - loss: 0.9510 - acc: 0.5138 - mse: 0.3436 - val_loss: 1.0213 - val_acc: 0.4600 - val_mse: 0.3637\n",
      "Epoch 17/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.1023 - acc: 0.3517 - mse: 0.4116\n",
      "Epoch 00017: val_loss improved from 1.02127 to 0.99610, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 361us/sample - loss: 0.9350 - acc: 0.5138 - mse: 0.3402 - val_loss: 0.9961 - val_acc: 0.4600 - val_mse: 0.3594\n",
      "Epoch 18/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.0826 - acc: 0.3517 - mse: 0.4065\n",
      "Epoch 00018: val_loss improved from 0.99610 to 0.97546, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 376us/sample - loss: 0.9214 - acc: 0.5125 - mse: 0.3368 - val_loss: 0.9755 - val_acc: 0.4600 - val_mse: 0.3552\n",
      "Epoch 19/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.0642 - acc: 0.3517 - mse: 0.4013\n",
      "Epoch 00019: val_loss improved from 0.97546 to 0.95742, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 416us/sample - loss: 0.9089 - acc: 0.5125 - mse: 0.3335 - val_loss: 0.9574 - val_acc: 0.4600 - val_mse: 0.3510\n",
      "Epoch 20/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.0466 - acc: 0.3500 - mse: 0.3960\n",
      "Epoch 00020: val_loss improved from 0.95742 to 0.94094, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 405us/sample - loss: 0.8972 - acc: 0.5113 - mse: 0.3302 - val_loss: 0.9409 - val_acc: 0.4600 - val_mse: 0.3469\n",
      "Epoch 21/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.3181 - acc: 0.0400 - mse: 0.5187\n",
      "Epoch 00021: val_loss improved from 0.94094 to 0.92538, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 376us/sample - loss: 0.8859 - acc: 0.5125 - mse: 0.3268 - val_loss: 0.9254 - val_acc: 0.4500 - val_mse: 0.3426\n",
      "Epoch 22/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 1.0121 - acc: 0.3517 - mse: 0.3847\n",
      "Epoch 00022: val_loss improved from 0.92538 to 0.91033, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 351us/sample - loss: 0.8747 - acc: 0.5100 - mse: 0.3232 - val_loss: 0.9103 - val_acc: 0.4450 - val_mse: 0.3383\n",
      "Epoch 23/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.9949 - acc: 0.3517 - mse: 0.3787\n",
      "Epoch 00023: val_loss improved from 0.91033 to 0.89547, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 419us/sample - loss: 0.8636 - acc: 0.5088 - mse: 0.3195 - val_loss: 0.8955 - val_acc: 0.4500 - val_mse: 0.3336\n",
      "Epoch 24/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.9775 - acc: 0.3517 - mse: 0.3724\n",
      "Epoch 00024: val_loss improved from 0.89547 to 0.88056, saving model to QuantLSTM.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 347us/sample - loss: 0.8522 - acc: 0.5088 - mse: 0.3156 - val_loss: 0.8806 - val_acc: 0.4500 - val_mse: 0.3287\n",
      "Epoch 25/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.1989 - acc: 0.0425 - mse: 0.4751\n",
      "Epoch 00025: val_loss improved from 0.88056 to 0.86539, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 0.8406 - acc: 0.5075 - mse: 0.3114 - val_loss: 0.8654 - val_acc: 0.4500 - val_mse: 0.3235\n",
      "Epoch 26/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.9413 - acc: 0.3517 - mse: 0.3587\n",
      "Epoch 00026: val_loss improved from 0.86539 to 0.84977, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 290us/sample - loss: 0.8285 - acc: 0.5088 - mse: 0.3069 - val_loss: 0.8498 - val_acc: 0.4500 - val_mse: 0.3177\n",
      "Epoch 27/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.1351 - acc: 0.0425 - mse: 0.4496\n",
      "Epoch 00027: val_loss improved from 0.84977 to 0.83350, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.8159 - acc: 0.5088 - mse: 0.3020 - val_loss: 0.8335 - val_acc: 0.4450 - val_mse: 0.3114\n",
      "Epoch 28/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.9021 - acc: 0.3533 - mse: 0.3429\n",
      "Epoch 00028: val_loss improved from 0.83350 to 0.81641, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.8024 - acc: 0.5100 - mse: 0.2965 - val_loss: 0.8164 - val_acc: 0.4500 - val_mse: 0.3045\n",
      "Epoch 29/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.8807 - acc: 0.3517 - mse: 0.3338\n",
      "Epoch 00029: val_loss improved from 0.81641 to 0.79831, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 0.7879 - acc: 0.5125 - mse: 0.2905 - val_loss: 0.7983 - val_acc: 0.4600 - val_mse: 0.2968\n",
      "Epoch 30/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 1.0260 - acc: 0.0600 - mse: 0.4029\n",
      "Epoch 00030: val_loss improved from 0.79831 to 0.77910, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 301us/sample - loss: 0.7720 - acc: 0.5238 - mse: 0.2836 - val_loss: 0.7791 - val_acc: 0.4700 - val_mse: 0.2883\n",
      "Epoch 31/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.8318 - acc: 0.4017 - mse: 0.3121\n",
      "Epoch 00031: val_loss improved from 0.77910 to 0.75882, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.7544 - acc: 0.5487 - mse: 0.2757 - val_loss: 0.7588 - val_acc: 0.5150 - val_mse: 0.2790\n",
      "Epoch 32/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.8030 - acc: 0.4500 - mse: 0.2990\n",
      "Epoch 00032: val_loss improved from 0.75882 to 0.73802, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 370us/sample - loss: 0.7345 - acc: 0.5800 - mse: 0.2667 - val_loss: 0.7380 - val_acc: 0.5600 - val_mse: 0.2693\n",
      "Epoch 33/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.7703 - acc: 0.5000 - mse: 0.2838\n",
      "Epoch 00033: val_loss improved from 0.73802 to 0.71863, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 0.7121 - acc: 0.6100 - mse: 0.2563 - val_loss: 0.7186 - val_acc: 0.5900 - val_mse: 0.2602\n",
      "Epoch 34/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.7335 - acc: 0.5383 - mse: 0.2671\n",
      "Epoch 00034: val_loss improved from 0.71863 to 0.70602, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 371us/sample - loss: 0.6871 - acc: 0.6275 - mse: 0.2450 - val_loss: 0.7060 - val_acc: 0.6100 - val_mse: 0.2537\n",
      "Epoch 35/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.7401 - acc: 0.4425 - mse: 0.2734\n",
      "Epoch 00035: val_loss did not improve from 0.70602\n",
      "800/800 [==============================] - 0s 346us/sample - loss: 0.6616 - acc: 0.6463 - mse: 0.2339 - val_loss: 0.7140 - val_acc: 0.6250 - val_mse: 0.2523\n",
      "Epoch 36/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6613 - acc: 0.5900 - mse: 0.2368\n",
      "Epoch 00036: val_loss did not improve from 0.70602\n",
      "800/800 [==============================] - 0s 340us/sample - loss: 0.6394 - acc: 0.6600 - mse: 0.2239 - val_loss: 0.9119 - val_acc: 0.6350 - val_mse: 0.2541\n",
      "Epoch 37/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.6294 - acc: 0.4925 - mse: 0.2361\n",
      "Epoch 00037: val_loss did not improve from 0.70602\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 0.6131 - acc: 0.6812 - mse: 0.2117 - val_loss: 0.7958 - val_acc: 0.6350 - val_mse: 0.2465\n",
      "Epoch 38/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6253 - acc: 0.6050 - mse: 0.2245\n",
      "Epoch 00038: val_loss improved from 0.70602 to 0.68016, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 0.5850 - acc: 0.6875 - mse: 0.2027 - val_loss: 0.6802 - val_acc: 0.6400 - val_mse: 0.2376\n",
      "Epoch 39/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6421 - acc: 0.5933 - mse: 0.2306\n",
      "Epoch 00039: val_loss improved from 0.68016 to 0.66947, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 0.5911 - acc: 0.6837 - mse: 0.2044 - val_loss: 0.6695 - val_acc: 0.6400 - val_mse: 0.2340\n",
      "Epoch 40/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6420 - acc: 0.5967 - mse: 0.2308\n",
      "Epoch 00040: val_loss did not improve from 0.66947\n",
      "800/800 [==============================] - 0s 403us/sample - loss: 0.5897 - acc: 0.6862 - mse: 0.2037 - val_loss: 0.6824 - val_acc: 0.6350 - val_mse: 0.2336\n",
      "Epoch 41/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6300 - acc: 0.6000 - mse: 0.2261\n",
      "Epoch 00041: val_loss did not improve from 0.66947\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 0.5832 - acc: 0.6862 - mse: 0.2006 - val_loss: 0.8563 - val_acc: 0.6500 - val_mse: 0.2355\n",
      "Epoch 42/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6172 - acc: 0.6117 - mse: 0.2203\n",
      "Epoch 00042: val_loss did not improve from 0.66947\n",
      "800/800 [==============================] - 0s 266us/sample - loss: 0.5890 - acc: 0.6938 - mse: 0.1967 - val_loss: 0.8599 - val_acc: 0.6450 - val_mse: 0.2352\n",
      "Epoch 43/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6042 - acc: 0.6183 - mse: 0.2156\n",
      "Epoch 00043: val_loss did not improve from 0.66947\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 0.5669 - acc: 0.7000 - mse: 0.1928 - val_loss: 0.6703 - val_acc: 0.6500 - val_mse: 0.2288\n",
      "Epoch 44/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5958 - acc: 0.6150 - mse: 0.2126\n",
      "Epoch 00044: val_loss improved from 0.66947 to 0.63169, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.5526 - acc: 0.7013 - mse: 0.1893 - val_loss: 0.6317 - val_acc: 0.6550 - val_mse: 0.2209\n",
      "Epoch 45/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6119 - acc: 0.6150 - mse: 0.2163\n",
      "Epoch 00045: val_loss improved from 0.63169 to 0.62559, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 0.5650 - acc: 0.7013 - mse: 0.1923 - val_loss: 0.6256 - val_acc: 0.6600 - val_mse: 0.2185\n",
      "Epoch 46/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6088 - acc: 0.6183 - mse: 0.2142\n",
      "Epoch 00046: val_loss did not improve from 0.62559\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 0.5651 - acc: 0.7038 - mse: 0.1918 - val_loss: 0.6263 - val_acc: 0.6700 - val_mse: 0.2180\n",
      "Epoch 47/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5949 - acc: 0.6317 - mse: 0.2084\n",
      "Epoch 00047: val_loss did not improve from 0.62559\n",
      "800/800 [==============================] - 0s 296us/sample - loss: 0.5586 - acc: 0.7088 - mse: 0.1890 - val_loss: 0.6340 - val_acc: 0.6700 - val_mse: 0.2191\n",
      "Epoch 48/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5790 - acc: 0.6517 - mse: 0.2018\n",
      "Epoch 00048: val_loss did not improve from 0.62559\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.5514 - acc: 0.7212 - mse: 0.1858 - val_loss: 0.6491 - val_acc: 0.6900 - val_mse: 0.2211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5665 - acc: 0.6633 - mse: 0.1962\n",
      "Epoch 00049: val_loss did not improve from 0.62559\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 0.5457 - acc: 0.7275 - mse: 0.1826 - val_loss: 0.6578 - val_acc: 0.6900 - val_mse: 0.2211\n",
      "Epoch 50/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5534 - acc: 0.6717 - mse: 0.1916\n",
      "Epoch 00050: val_loss did not improve from 0.62559\n",
      "800/800 [==============================] - 0s 293us/sample - loss: 0.5318 - acc: 0.7400 - mse: 0.1775 - val_loss: 0.6437 - val_acc: 0.6950 - val_mse: 0.2180\n",
      "Epoch 51/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5485 - acc: 0.6700 - mse: 0.1905\n",
      "Epoch 00051: val_loss did not improve from 0.62559\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 0.5216 - acc: 0.7387 - mse: 0.1743 - val_loss: 0.6271 - val_acc: 0.6900 - val_mse: 0.2143\n",
      "Epoch 52/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5511 - acc: 0.6700 - mse: 0.1913\n",
      "Epoch 00052: val_loss improved from 0.62559 to 0.61895, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 374us/sample - loss: 0.5194 - acc: 0.7387 - mse: 0.1733 - val_loss: 0.6190 - val_acc: 0.7000 - val_mse: 0.2119\n",
      "Epoch 53/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5518 - acc: 0.6733 - mse: 0.1912\n",
      "Epoch 00053: val_loss improved from 0.61895 to 0.61836, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.5181 - acc: 0.7412 - mse: 0.1724 - val_loss: 0.6184 - val_acc: 0.6950 - val_mse: 0.2108\n",
      "Epoch 54/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5811 - acc: 0.5675 - mse: 0.2150\n",
      "Epoch 00054: val_loss did not improve from 0.61836\n",
      "800/800 [==============================] - 0s 279us/sample - loss: 0.5165 - acc: 0.7475 - mse: 0.1713 - val_loss: 0.6236 - val_acc: 0.7000 - val_mse: 0.2104\n",
      "Epoch 55/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5437 - acc: 0.6867 - mse: 0.1877\n",
      "Epoch 00055: val_loss did not improve from 0.61836\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.5145 - acc: 0.7513 - mse: 0.1699 - val_loss: 0.6240 - val_acc: 0.7200 - val_mse: 0.2092\n",
      "Epoch 56/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5366 - acc: 0.6950 - mse: 0.1853\n",
      "Epoch 00056: val_loss improved from 0.61836 to 0.61336, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.5078 - acc: 0.7575 - mse: 0.1682 - val_loss: 0.6134 - val_acc: 0.7100 - val_mse: 0.2069\n",
      "Epoch 57/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5352 - acc: 0.7000 - mse: 0.1841\n",
      "Epoch 00057: val_loss improved from 0.61336 to 0.60814, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 0.5076 - acc: 0.7613 - mse: 0.1678 - val_loss: 0.6081 - val_acc: 0.7100 - val_mse: 0.2050\n",
      "Epoch 58/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5320 - acc: 0.7150 - mse: 0.1822\n",
      "Epoch 00058: val_loss improved from 0.60814 to 0.60524, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 306us/sample - loss: 0.5074 - acc: 0.7713 - mse: 0.1673 - val_loss: 0.6052 - val_acc: 0.7200 - val_mse: 0.2031\n",
      "Epoch 59/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5271 - acc: 0.7233 - mse: 0.1799\n",
      "Epoch 00059: val_loss improved from 0.60524 to 0.60286, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.5049 - acc: 0.7775 - mse: 0.1660 - val_loss: 0.6029 - val_acc: 0.7100 - val_mse: 0.2018\n",
      "Epoch 60/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5358 - acc: 0.6450 - mse: 0.1949\n",
      "Epoch 00060: val_loss improved from 0.60286 to 0.60071, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 343us/sample - loss: 0.5021 - acc: 0.7825 - mse: 0.1648 - val_loss: 0.6007 - val_acc: 0.7100 - val_mse: 0.2010\n",
      "Epoch 61/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5345 - acc: 0.6550 - mse: 0.1940\n",
      "Epoch 00061: val_loss improved from 0.60071 to 0.59905, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 345us/sample - loss: 0.5016 - acc: 0.7875 - mse: 0.1644 - val_loss: 0.5990 - val_acc: 0.7100 - val_mse: 0.1997\n",
      "Epoch 62/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5201 - acc: 0.7400 - mse: 0.1766\n",
      "Epoch 00062: val_loss improved from 0.59905 to 0.59784, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 317us/sample - loss: 0.5009 - acc: 0.7900 - mse: 0.1639 - val_loss: 0.5978 - val_acc: 0.7200 - val_mse: 0.1982\n",
      "Epoch 63/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5172 - acc: 0.7417 - mse: 0.1753\n",
      "Epoch 00063: val_loss improved from 0.59784 to 0.59483, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 0.4986 - acc: 0.7887 - mse: 0.1629 - val_loss: 0.5948 - val_acc: 0.7100 - val_mse: 0.1972\n",
      "Epoch 64/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5160 - acc: 0.7450 - mse: 0.1748\n",
      "Epoch 00064: val_loss improved from 0.59483 to 0.59093, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 0.4963 - acc: 0.7912 - mse: 0.1620 - val_loss: 0.5909 - val_acc: 0.7100 - val_mse: 0.1962\n",
      "Epoch 65/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5297 - acc: 0.6700 - mse: 0.1910\n",
      "Epoch 00065: val_loss improved from 0.59093 to 0.58921, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 319us/sample - loss: 0.4961 - acc: 0.7912 - mse: 0.1618 - val_loss: 0.5892 - val_acc: 0.7100 - val_mse: 0.1951\n",
      "Epoch 66/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5163 - acc: 0.7467 - mse: 0.1747\n",
      "Epoch 00066: val_loss improved from 0.58921 to 0.58649, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 279us/sample - loss: 0.4952 - acc: 0.7912 - mse: 0.1614 - val_loss: 0.5865 - val_acc: 0.7150 - val_mse: 0.1938\n",
      "Epoch 67/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5160 - acc: 0.7467 - mse: 0.1746\n",
      "Epoch 00067: val_loss improved from 0.58649 to 0.58196, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 0.4936 - acc: 0.7912 - mse: 0.1608 - val_loss: 0.5820 - val_acc: 0.7300 - val_mse: 0.1918\n",
      "Epoch 68/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5334 - acc: 0.6700 - mse: 0.1922\n",
      "Epoch 00068: val_loss improved from 0.58196 to 0.58043, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 366us/sample - loss: 0.4936 - acc: 0.7912 - mse: 0.1608 - val_loss: 0.5804 - val_acc: 0.7300 - val_mse: 0.1910\n",
      "Epoch 69/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5164 - acc: 0.7483 - mse: 0.1746\n",
      "Epoch 00069: val_loss improved from 0.58043 to 0.57827, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 324us/sample - loss: 0.4927 - acc: 0.7925 - mse: 0.1604 - val_loss: 0.5783 - val_acc: 0.7350 - val_mse: 0.1905\n",
      "Epoch 70/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5161 - acc: 0.7483 - mse: 0.1745\n",
      "Epoch 00070: val_loss improved from 0.57827 to 0.57647, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 267us/sample - loss: 0.4917 - acc: 0.7925 - mse: 0.1601 - val_loss: 0.5765 - val_acc: 0.7300 - val_mse: 0.1899\n",
      "Epoch 71/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5163 - acc: 0.7483 - mse: 0.1744\n",
      "Epoch 00071: val_loss improved from 0.57647 to 0.57468, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 298us/sample - loss: 0.4917 - acc: 0.7912 - mse: 0.1600 - val_loss: 0.5747 - val_acc: 0.7250 - val_mse: 0.1894\n",
      "Epoch 72/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5151 - acc: 0.7500 - mse: 0.1739\n",
      "Epoch 00072: val_loss improved from 0.57468 to 0.57329, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 270us/sample - loss: 0.4902 - acc: 0.7925 - mse: 0.1594 - val_loss: 0.5733 - val_acc: 0.7300 - val_mse: 0.1888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5153 - acc: 0.7517 - mse: 0.1737\n",
      "Epoch 00073: val_loss improved from 0.57329 to 0.57174, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 374us/sample - loss: 0.4912 - acc: 0.7900 - mse: 0.1596 - val_loss: 0.5717 - val_acc: 0.7300 - val_mse: 0.1879\n",
      "Epoch 74/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5122 - acc: 0.7550 - mse: 0.1723\n",
      "Epoch 00074: val_loss improved from 0.57174 to 0.56838, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 0.4880 - acc: 0.7925 - mse: 0.1583 - val_loss: 0.5684 - val_acc: 0.7300 - val_mse: 0.1872\n",
      "Epoch 75/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5111 - acc: 0.7583 - mse: 0.1718\n",
      "Epoch 00075: val_loss improved from 0.56838 to 0.56707, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.4865 - acc: 0.7937 - mse: 0.1577 - val_loss: 0.5671 - val_acc: 0.7350 - val_mse: 0.1863\n",
      "Epoch 76/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5249 - acc: 0.6875 - mse: 0.1876\n",
      "Epoch 00076: val_loss improved from 0.56707 to 0.56661, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 347us/sample - loss: 0.4861 - acc: 0.7900 - mse: 0.1574 - val_loss: 0.5666 - val_acc: 0.7400 - val_mse: 0.1855\n",
      "Epoch 77/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5069 - acc: 0.7567 - mse: 0.1698\n",
      "Epoch 00077: val_loss improved from 0.56661 to 0.56494, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 437us/sample - loss: 0.4822 - acc: 0.7912 - mse: 0.1558 - val_loss: 0.5649 - val_acc: 0.7400 - val_mse: 0.1844\n",
      "Epoch 78/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5061 - acc: 0.7567 - mse: 0.1692\n",
      "Epoch 00078: val_loss improved from 0.56494 to 0.55948, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 407us/sample - loss: 0.4789 - acc: 0.7950 - mse: 0.1544 - val_loss: 0.5595 - val_acc: 0.7400 - val_mse: 0.1832\n",
      "Epoch 79/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5073 - acc: 0.7567 - mse: 0.1701\n",
      "Epoch 00079: val_loss improved from 0.55948 to 0.55926, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.4752 - acc: 0.7962 - mse: 0.1533 - val_loss: 0.5593 - val_acc: 0.7650 - val_mse: 0.1823\n",
      "Epoch 80/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5109 - acc: 0.7633 - mse: 0.1705\n",
      "Epoch 00080: val_loss did not improve from 0.55926\n",
      "800/800 [==============================] - 0s 282us/sample - loss: 0.4815 - acc: 0.7975 - mse: 0.1549 - val_loss: 0.5616 - val_acc: 0.7800 - val_mse: 0.1781\n",
      "Epoch 81/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5126 - acc: 0.7000 - mse: 0.1831\n",
      "Epoch 00081: val_loss improved from 0.55926 to 0.55715, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 0.4774 - acc: 0.7987 - mse: 0.1530 - val_loss: 0.5571 - val_acc: 0.7650 - val_mse: 0.1775\n",
      "Epoch 82/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.4605 - acc: 0.7475 - mse: 0.1613\n",
      "Epoch 00082: val_loss did not improve from 0.55715\n",
      "800/800 [==============================] - 0s 308us/sample - loss: 0.4930 - acc: 0.8025 - mse: 0.1563 - val_loss: 0.5607 - val_acc: 0.7600 - val_mse: 0.1827\n",
      "Epoch 83/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.4193 - acc: 0.7775 - mse: 0.1422\n",
      "Epoch 00083: val_loss improved from 0.55715 to 0.54177, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 326us/sample - loss: 0.4452 - acc: 0.8250 - mse: 0.1401 - val_loss: 0.5418 - val_acc: 0.7750 - val_mse: 0.1784\n",
      "Epoch 84/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4449 - acc: 0.8083 - mse: 0.1414\n",
      "Epoch 00084: val_loss improved from 0.54177 to 0.54009, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 0.4266 - acc: 0.8338 - mse: 0.1317 - val_loss: 0.5401 - val_acc: 0.7650 - val_mse: 0.1780\n",
      "Epoch 85/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.4773 - acc: 0.7600 - mse: 0.1588\n",
      "Epoch 00085: val_loss improved from 0.54009 to 0.53966, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 0.5265 - acc: 0.7837 - mse: 0.1689 - val_loss: 0.5397 - val_acc: 0.7900 - val_mse: 0.1719\n",
      "Epoch 86/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4265 - acc: 0.8467 - mse: 0.1283\n",
      "Epoch 00086: val_loss did not improve from 0.53966\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.5307 - acc: 0.7987 - mse: 0.1673 - val_loss: 0.5499 - val_acc: 0.7700 - val_mse: 0.1809\n",
      "Epoch 87/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3702 - acc: 0.8617 - mse: 0.1099\n",
      "Epoch 00087: val_loss improved from 0.53966 to 0.47787, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 383us/sample - loss: 0.4051 - acc: 0.8338 - mse: 0.1252 - val_loss: 0.4779 - val_acc: 0.8000 - val_mse: 0.1537\n",
      "Epoch 88/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4059 - acc: 0.8617 - mse: 0.1203\n",
      "Epoch 00088: val_loss improved from 0.47787 to 0.46830, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 0.3859 - acc: 0.8612 - mse: 0.1139 - val_loss: 0.4683 - val_acc: 0.8100 - val_mse: 0.1502\n",
      "Epoch 89/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.4902 - acc: 0.8750 - mse: 0.1477\n",
      "Epoch 00089: val_loss improved from 0.46830 to 0.45640, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 306us/sample - loss: 0.4026 - acc: 0.8612 - mse: 0.1211 - val_loss: 0.4564 - val_acc: 0.8150 - val_mse: 0.1462\n",
      "Epoch 90/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5383 - acc: 0.8450 - mse: 0.1648\n",
      "Epoch 00090: val_loss did not improve from 0.45640\n",
      "800/800 [==============================] - 0s 297us/sample - loss: 0.4216 - acc: 0.8500 - mse: 0.1288 - val_loss: 0.4598 - val_acc: 0.8100 - val_mse: 0.1484\n",
      "Epoch 91/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4700 - acc: 0.8300 - mse: 0.1457\n",
      "Epoch 00091: val_loss did not improve from 0.45640\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 0.4384 - acc: 0.8325 - mse: 0.1363 - val_loss: 0.4656 - val_acc: 0.8100 - val_mse: 0.1506\n",
      "Epoch 92/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4288 - acc: 0.8517 - mse: 0.1280\n",
      "Epoch 00092: val_loss improved from 0.45640 to 0.41946, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 362us/sample - loss: 0.3749 - acc: 0.8675 - mse: 0.1116 - val_loss: 0.4195 - val_acc: 0.8550 - val_mse: 0.1258\n",
      "Epoch 93/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.7855 - acc: 0.7525 - mse: 0.2141\n",
      "Epoch 00093: val_loss improved from 0.41946 to 0.40535, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 346us/sample - loss: 0.4978 - acc: 0.8400 - mse: 0.1362 - val_loss: 0.4053 - val_acc: 0.8600 - val_mse: 0.1218\n",
      "Epoch 94/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5265 - acc: 0.8017 - mse: 0.1619\n",
      "Epoch 00094: val_loss improved from 0.40535 to 0.39986, saving model to QuantLSTM.h5\n",
      "800/800 [==============================] - 0s 284us/sample - loss: 0.4472 - acc: 0.8400 - mse: 0.1333 - val_loss: 0.3999 - val_acc: 0.8600 - val_mse: 0.1206\n",
      "Epoch 95/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4862 - acc: 0.8033 - mse: 0.1533\n",
      "Epoch 00095: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 274us/sample - loss: 0.4278 - acc: 0.8413 - mse: 0.1296 - val_loss: 0.4041 - val_acc: 0.8600 - val_mse: 0.1215\n",
      "Epoch 96/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4574 - acc: 0.8050 - mse: 0.1451\n",
      "Epoch 00096: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.4158 - acc: 0.8425 - mse: 0.1261 - val_loss: 0.4222 - val_acc: 0.8550 - val_mse: 0.1267\n",
      "Epoch 97/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4230 - acc: 0.8200 - mse: 0.1330\n",
      "Epoch 00097: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 262us/sample - loss: 0.4088 - acc: 0.8475 - mse: 0.1236 - val_loss: 0.5188 - val_acc: 0.8350 - val_mse: 0.1405\n",
      "Epoch 98/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3919 - acc: 0.8383 - mse: 0.1212\n",
      "Epoch 00098: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 269us/sample - loss: 0.3937 - acc: 0.8612 - mse: 0.1171 - val_loss: 0.5320 - val_acc: 0.8300 - val_mse: 0.1455\n",
      "Epoch 99/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3737 - acc: 0.8567 - mse: 0.1142\n",
      "Epoch 00099: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 265us/sample - loss: 0.3831 - acc: 0.8725 - mse: 0.1133 - val_loss: 0.4752 - val_acc: 0.8300 - val_mse: 0.1446\n",
      "Epoch 100/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3670 - acc: 0.8617 - mse: 0.1115\n",
      "Epoch 00100: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 273us/sample - loss: 0.3751 - acc: 0.8763 - mse: 0.1107 - val_loss: 0.4621 - val_acc: 0.8300 - val_mse: 0.1431\n",
      "Epoch 101/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.3290 - acc: 0.8450 - mse: 0.1041\n",
      "Epoch 00101: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 0.3738 - acc: 0.8775 - mse: 0.1100 - val_loss: 0.4586 - val_acc: 0.8300 - val_mse: 0.1421\n",
      "Epoch 102/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3817 - acc: 0.8550 - mse: 0.1163\n",
      "Epoch 00102: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 276us/sample - loss: 0.3962 - acc: 0.8637 - mse: 0.1187 - val_loss: 0.4980 - val_acc: 0.8100 - val_mse: 0.1571\n",
      "Epoch 103/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.3076 - acc: 0.8625 - mse: 0.0944\n",
      "Epoch 00103: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 288us/sample - loss: 0.4287 - acc: 0.8413 - mse: 0.1317 - val_loss: 0.5137 - val_acc: 0.7950 - val_mse: 0.1647\n",
      "Epoch 104/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.3203 - acc: 0.8625 - mse: 0.0972\n",
      "Epoch 00104: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 271us/sample - loss: 0.4074 - acc: 0.8462 - mse: 0.1250 - val_loss: 0.5000 - val_acc: 0.7950 - val_mse: 0.1613\n",
      "Epoch 105/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3774 - acc: 0.8583 - mse: 0.1134\n",
      "Epoch 00105: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 269us/sample - loss: 0.3858 - acc: 0.8587 - mse: 0.1158 - val_loss: 0.4787 - val_acc: 0.8100 - val_mse: 0.1522\n",
      "Epoch 106/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3758 - acc: 0.8633 - mse: 0.1118\n",
      "Epoch 00106: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 266us/sample - loss: 0.3675 - acc: 0.8725 - mse: 0.1074 - val_loss: 0.4726 - val_acc: 0.8200 - val_mse: 0.1472\n",
      "Epoch 107/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.4075 - acc: 0.8375 - mse: 0.1275\n",
      "Epoch 00107: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 292us/sample - loss: 0.3939 - acc: 0.8650 - mse: 0.1128 - val_loss: 0.4927 - val_acc: 0.8300 - val_mse: 0.1448\n",
      "Epoch 108/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3993 - acc: 0.8483 - mse: 0.1221\n",
      "Epoch 00108: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 319us/sample - loss: 0.3901 - acc: 0.8650 - mse: 0.1117 - val_loss: 0.6574 - val_acc: 0.8350 - val_mse: 0.1445\n",
      "Epoch 109/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.3995 - acc: 0.8483 - mse: 0.1219\n",
      "Epoch 00109: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 323us/sample - loss: 0.3887 - acc: 0.8662 - mse: 0.1110 - val_loss: 0.6609 - val_acc: 0.8350 - val_mse: 0.1443\n",
      "Epoch 110/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4202 - acc: 0.8350 - mse: 0.1312\n",
      "Epoch 00110: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 247us/sample - loss: 0.5116 - acc: 0.7900 - mse: 0.1616 - val_loss: 0.5928 - val_acc: 0.7550 - val_mse: 0.1894\n",
      "Epoch 111/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5058 - acc: 0.7633 - mse: 0.1708\n",
      "Epoch 00111: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 296us/sample - loss: 0.6545 - acc: 0.6538 - mse: 0.2338 - val_loss: 0.6809 - val_acc: 0.6050 - val_mse: 0.2453\n",
      "Epoch 112/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5043 - acc: 0.7183 - mse: 0.1702\n",
      "Epoch 00112: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 293us/sample - loss: 0.5901 - acc: 0.5900 - mse: 0.2097 - val_loss: 0.6061 - val_acc: 0.5750 - val_mse: 0.2135\n",
      "Epoch 113/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5243 - acc: 0.7717 - mse: 0.1731\n",
      "Epoch 00113: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 238us/sample - loss: 0.5361 - acc: 0.7600 - mse: 0.1785 - val_loss: 0.5589 - val_acc: 0.7650 - val_mse: 0.1905\n",
      "Epoch 114/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5989 - acc: 0.6733 - mse: 0.2072\n",
      "Epoch 00114: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.5758 - acc: 0.7475 - mse: 0.1952 - val_loss: 0.5897 - val_acc: 0.6700 - val_mse: 0.2043\n",
      "Epoch 115/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6179 - acc: 0.6033 - mse: 0.2182\n",
      "Epoch 00115: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 0.5799 - acc: 0.6950 - mse: 0.1984 - val_loss: 0.6078 - val_acc: 0.6400 - val_mse: 0.2107\n",
      "Epoch 116/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6276 - acc: 0.5967 - mse: 0.2243\n",
      "Epoch 00116: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 253us/sample - loss: 0.5880 - acc: 0.6900 - mse: 0.2026 - val_loss: 0.7952 - val_acc: 0.6500 - val_mse: 0.2145\n",
      "Epoch 117/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6148 - acc: 0.6050 - mse: 0.2200\n",
      "Epoch 00117: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 254us/sample - loss: 0.5971 - acc: 0.6888 - mse: 0.2015 - val_loss: 0.8091 - val_acc: 0.6550 - val_mse: 0.2205\n",
      "Epoch 118/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6367 - acc: 0.6233 - mse: 0.2132\n",
      "Epoch 00118: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 253us/sample - loss: 0.6204 - acc: 0.6988 - mse: 0.1995 - val_loss: 0.8843 - val_acc: 0.6600 - val_mse: 0.2280\n",
      "Epoch 119/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6393 - acc: 0.6450 - mse: 0.2078\n",
      "Epoch 00119: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 275us/sample - loss: 0.6259 - acc: 0.7113 - mse: 0.1966 - val_loss: 0.9516 - val_acc: 0.6650 - val_mse: 0.2312\n",
      "Epoch 120/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6291 - acc: 0.6483 - mse: 0.2043\n",
      "Epoch 00120: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 299us/sample - loss: 0.6142 - acc: 0.7163 - mse: 0.1923 - val_loss: 0.8871 - val_acc: 0.6700 - val_mse: 0.2275\n",
      "Epoch 121/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6114 - acc: 0.6367 - mse: 0.2032\n",
      "Epoch 00121: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 439us/sample - loss: 0.5904 - acc: 0.7138 - mse: 0.1871 - val_loss: 0.7855 - val_acc: 0.6650 - val_mse: 0.2097\n",
      "Epoch 122/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.6528 - acc: 0.4675 - mse: 0.2455\n",
      "Epoch 00122: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.5486 - acc: 0.7088 - mse: 0.1873 - val_loss: 0.5810 - val_acc: 0.6600 - val_mse: 0.2011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.7131 - acc: 0.4300 - mse: 0.2669\n",
      "Epoch 00123: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 0.5768 - acc: 0.6963 - mse: 0.1975 - val_loss: 0.5740 - val_acc: 0.6900 - val_mse: 0.1982\n",
      "Epoch 124/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6244 - acc: 0.6017 - mse: 0.2209\n",
      "Epoch 00124: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 291us/sample - loss: 0.5840 - acc: 0.6938 - mse: 0.2001 - val_loss: 0.5701 - val_acc: 0.7000 - val_mse: 0.1963\n",
      "Epoch 125/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.6587 - acc: 0.4825 - mse: 0.2406\n",
      "Epoch 00125: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 0.5786 - acc: 0.7075 - mse: 0.1976 - val_loss: 0.5701 - val_acc: 0.7100 - val_mse: 0.1961\n",
      "Epoch 126/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5827 - acc: 0.6550 - mse: 0.2026\n",
      "Epoch 00126: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 280us/sample - loss: 0.5684 - acc: 0.7262 - mse: 0.1934 - val_loss: 0.5767 - val_acc: 0.7200 - val_mse: 0.1981\n",
      "Epoch 127/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5619 - acc: 0.6800 - mse: 0.1945\n",
      "Epoch 00127: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 271us/sample - loss: 0.5715 - acc: 0.7362 - mse: 0.1891 - val_loss: 0.5936 - val_acc: 0.7200 - val_mse: 0.2016\n",
      "Epoch 128/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5483 - acc: 0.6900 - mse: 0.1897\n",
      "Epoch 00128: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 263us/sample - loss: 0.5611 - acc: 0.7437 - mse: 0.1854 - val_loss: 0.7065 - val_acc: 0.7150 - val_mse: 0.2056\n",
      "Epoch 129/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5448 - acc: 0.6883 - mse: 0.1879\n",
      "Epoch 00129: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 247us/sample - loss: 0.5533 - acc: 0.7425 - mse: 0.1818 - val_loss: 0.8416 - val_acc: 0.7150 - val_mse: 0.2061\n",
      "Epoch 130/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5430 - acc: 0.6733 - mse: 0.1887\n",
      "Epoch 00130: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 268us/sample - loss: 0.5425 - acc: 0.7325 - mse: 0.1781 - val_loss: 0.6413 - val_acc: 0.7250 - val_mse: 0.2035\n",
      "Epoch 131/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5516 - acc: 0.6650 - mse: 0.1927\n",
      "Epoch 00131: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 265us/sample - loss: 0.5413 - acc: 0.7312 - mse: 0.1777 - val_loss: 0.6118 - val_acc: 0.7050 - val_mse: 0.2017\n",
      "Epoch 132/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5928 - acc: 0.5550 - mse: 0.2198\n",
      "Epoch 00132: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 289us/sample - loss: 0.5445 - acc: 0.7337 - mse: 0.1790 - val_loss: 0.6107 - val_acc: 0.7050 - val_mse: 0.2011\n",
      "Epoch 133/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5634 - acc: 0.6633 - mse: 0.1974\n",
      "Epoch 00133: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 280us/sample - loss: 0.5468 - acc: 0.7312 - mse: 0.1798 - val_loss: 0.6233 - val_acc: 0.7100 - val_mse: 0.2011\n",
      "Epoch 134/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5604 - acc: 0.6667 - mse: 0.1958\n",
      "Epoch 00134: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 0.5467 - acc: 0.7337 - mse: 0.1797 - val_loss: 0.6594 - val_acc: 0.7100 - val_mse: 0.2011\n",
      "Epoch 135/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5544 - acc: 0.6767 - mse: 0.1931\n",
      "Epoch 00135: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 325us/sample - loss: 0.5445 - acc: 0.7412 - mse: 0.1786 - val_loss: 0.6397 - val_acc: 0.7200 - val_mse: 0.2002\n",
      "Epoch 136/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5481 - acc: 0.6850 - mse: 0.1905\n",
      "Epoch 00136: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 273us/sample - loss: 0.5410 - acc: 0.7475 - mse: 0.1773 - val_loss: 0.6135 - val_acc: 0.7250 - val_mse: 0.1987\n",
      "Epoch 137/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5444 - acc: 0.6933 - mse: 0.1889\n",
      "Epoch 00137: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 251us/sample - loss: 0.5387 - acc: 0.7500 - mse: 0.1763 - val_loss: 0.6037 - val_acc: 0.7250 - val_mse: 0.1976\n",
      "Epoch 138/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5519 - acc: 0.6100 - mse: 0.2014\n",
      "Epoch 00138: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 306us/sample - loss: 0.5371 - acc: 0.7525 - mse: 0.1755 - val_loss: 0.6010 - val_acc: 0.7300 - val_mse: 0.1969\n",
      "Epoch 139/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5409 - acc: 0.7000 - mse: 0.1870\n",
      "Epoch 00139: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 341us/sample - loss: 0.5356 - acc: 0.7513 - mse: 0.1748 - val_loss: 0.6018 - val_acc: 0.7350 - val_mse: 0.1967\n",
      "Epoch 140/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5400 - acc: 0.6983 - mse: 0.1865\n",
      "Epoch 00140: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 355us/sample - loss: 0.5342 - acc: 0.7487 - mse: 0.1741 - val_loss: 0.6030 - val_acc: 0.7350 - val_mse: 0.1964\n",
      "Epoch 141/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5395 - acc: 0.7000 - mse: 0.1861\n",
      "Epoch 00141: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 0.5329 - acc: 0.7500 - mse: 0.1736 - val_loss: 0.6020 - val_acc: 0.7350 - val_mse: 0.1960\n",
      "Epoch 142/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5391 - acc: 0.7017 - mse: 0.1859\n",
      "Epoch 00142: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 0.5318 - acc: 0.7513 - mse: 0.1731 - val_loss: 0.5988 - val_acc: 0.7350 - val_mse: 0.1954\n",
      "Epoch 143/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5388 - acc: 0.7083 - mse: 0.1856\n",
      "Epoch 00143: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 338us/sample - loss: 0.5311 - acc: 0.7550 - mse: 0.1727 - val_loss: 0.5956 - val_acc: 0.7300 - val_mse: 0.1948\n",
      "Epoch 144/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5382 - acc: 0.7067 - mse: 0.1852\n",
      "Epoch 00144: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 0.5305 - acc: 0.7525 - mse: 0.1724 - val_loss: 0.5932 - val_acc: 0.7300 - val_mse: 0.1943\n",
      "Epoch 145/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5373 - acc: 0.7150 - mse: 0.1846\n",
      "Epoch 00145: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 361us/sample - loss: 0.5300 - acc: 0.7588 - mse: 0.1721 - val_loss: 0.5921 - val_acc: 0.7300 - val_mse: 0.1938\n",
      "Epoch 146/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5359 - acc: 0.7217 - mse: 0.1839\n",
      "Epoch 00146: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 345us/sample - loss: 0.5292 - acc: 0.7638 - mse: 0.1718 - val_loss: 0.5914 - val_acc: 0.7350 - val_mse: 0.1935\n",
      "Epoch 147/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5442 - acc: 0.6575 - mse: 0.1960\n",
      "Epoch 00147: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 0.5282 - acc: 0.7625 - mse: 0.1713 - val_loss: 0.5903 - val_acc: 0.7400 - val_mse: 0.1931\n",
      "Epoch 148/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5331 - acc: 0.7217 - mse: 0.1826\n",
      "Epoch 00148: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 375us/sample - loss: 0.5270 - acc: 0.7613 - mse: 0.1708 - val_loss: 0.5888 - val_acc: 0.7400 - val_mse: 0.1927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5322 - acc: 0.7250 - mse: 0.1821\n",
      "Epoch 00149: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.5260 - acc: 0.7613 - mse: 0.1703 - val_loss: 0.5873 - val_acc: 0.7450 - val_mse: 0.1923\n",
      "Epoch 150/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5316 - acc: 0.7267 - mse: 0.1818\n",
      "Epoch 00150: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.5252 - acc: 0.7625 - mse: 0.1700 - val_loss: 0.5864 - val_acc: 0.7450 - val_mse: 0.1919\n",
      "Epoch 151/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5309 - acc: 0.7267 - mse: 0.1814\n",
      "Epoch 00151: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 284us/sample - loss: 0.5243 - acc: 0.7625 - mse: 0.1696 - val_loss: 0.5857 - val_acc: 0.7450 - val_mse: 0.1916\n",
      "Epoch 152/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5301 - acc: 0.7283 - mse: 0.1810\n",
      "Epoch 00152: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.5234 - acc: 0.7650 - mse: 0.1692 - val_loss: 0.5852 - val_acc: 0.7500 - val_mse: 0.1912\n",
      "Epoch 153/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5414 - acc: 0.6750 - mse: 0.1939\n",
      "Epoch 00153: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 303us/sample - loss: 0.5223 - acc: 0.7688 - mse: 0.1687 - val_loss: 0.5844 - val_acc: 0.7450 - val_mse: 0.1908\n",
      "Epoch 154/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5265 - acc: 0.7317 - mse: 0.1797\n",
      "Epoch 00154: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 410us/sample - loss: 0.5183 - acc: 0.7675 - mse: 0.1672 - val_loss: 0.5813 - val_acc: 0.7400 - val_mse: 0.1897\n",
      "Epoch 155/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5275 - acc: 0.7317 - mse: 0.1802\n",
      "Epoch 00155: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 397us/sample - loss: 0.5178 - acc: 0.7675 - mse: 0.1670 - val_loss: 0.5807 - val_acc: 0.7400 - val_mse: 0.1892\n",
      "Epoch 156/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5278 - acc: 0.7350 - mse: 0.1803\n",
      "Epoch 00156: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 354us/sample - loss: 0.5185 - acc: 0.7700 - mse: 0.1673 - val_loss: 0.5820 - val_acc: 0.7400 - val_mse: 0.1891\n",
      "Epoch 157/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5257 - acc: 0.7350 - mse: 0.1795\n",
      "Epoch 00157: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 306us/sample - loss: 0.5179 - acc: 0.7663 - mse: 0.1671 - val_loss: 0.5833 - val_acc: 0.7450 - val_mse: 0.1889\n",
      "Epoch 158/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5368 - acc: 0.6875 - mse: 0.1929\n",
      "Epoch 00158: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.5168 - acc: 0.7700 - mse: 0.1667 - val_loss: 0.5836 - val_acc: 0.7500 - val_mse: 0.1887\n",
      "Epoch 159/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5205 - acc: 0.7400 - mse: 0.1774\n",
      "Epoch 00159: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 282us/sample - loss: 0.5149 - acc: 0.7700 - mse: 0.1660 - val_loss: 0.5833 - val_acc: 0.7500 - val_mse: 0.1884\n",
      "Epoch 160/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5335 - acc: 0.6900 - mse: 0.1915\n",
      "Epoch 00160: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 335us/sample - loss: 0.5127 - acc: 0.7700 - mse: 0.1651 - val_loss: 0.5835 - val_acc: 0.7500 - val_mse: 0.1881\n",
      "Epoch 161/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5194 - acc: 0.7383 - mse: 0.1771\n",
      "Epoch 00161: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.5121 - acc: 0.7688 - mse: 0.1649 - val_loss: 0.5863 - val_acc: 0.7550 - val_mse: 0.1879\n",
      "Epoch 162/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5379 - acc: 0.6850 - mse: 0.1934\n",
      "Epoch 00162: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.5120 - acc: 0.7675 - mse: 0.1649 - val_loss: 0.6331 - val_acc: 0.7550 - val_mse: 0.1877\n",
      "Epoch 163/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5187 - acc: 0.7383 - mse: 0.1768\n",
      "Epoch 00163: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 376us/sample - loss: 0.5115 - acc: 0.7675 - mse: 0.1647 - val_loss: 0.6328 - val_acc: 0.7400 - val_mse: 0.1875\n",
      "Epoch 164/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5175 - acc: 0.7383 - mse: 0.1763\n",
      "Epoch 00164: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 370us/sample - loss: 0.5110 - acc: 0.7638 - mse: 0.1645 - val_loss: 0.6324 - val_acc: 0.7450 - val_mse: 0.1874\n",
      "Epoch 165/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5159 - acc: 0.7383 - mse: 0.1757\n",
      "Epoch 00165: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 383us/sample - loss: 0.5101 - acc: 0.7638 - mse: 0.1642 - val_loss: 0.6321 - val_acc: 0.7450 - val_mse: 0.1873\n",
      "Epoch 166/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5143 - acc: 0.7383 - mse: 0.1750\n",
      "Epoch 00166: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 365us/sample - loss: 0.5086 - acc: 0.7638 - mse: 0.1636 - val_loss: 0.6316 - val_acc: 0.7450 - val_mse: 0.1873\n",
      "Epoch 167/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5137 - acc: 0.7433 - mse: 0.1747\n",
      "Epoch 00167: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 252us/sample - loss: 0.5072 - acc: 0.7675 - mse: 0.1630 - val_loss: 0.6314 - val_acc: 0.7450 - val_mse: 0.1874\n",
      "Epoch 168/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5138 - acc: 0.7433 - mse: 0.1747\n",
      "Epoch 00168: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 319us/sample - loss: 0.5064 - acc: 0.7675 - mse: 0.1626 - val_loss: 0.6314 - val_acc: 0.7450 - val_mse: 0.1875\n",
      "Epoch 169/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5136 - acc: 0.7400 - mse: 0.1746\n",
      "Epoch 00169: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 355us/sample - loss: 0.5059 - acc: 0.7650 - mse: 0.1623 - val_loss: 0.6315 - val_acc: 0.7450 - val_mse: 0.1876\n",
      "Epoch 170/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5125 - acc: 0.7383 - mse: 0.1741\n",
      "Epoch 00170: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 333us/sample - loss: 0.5048 - acc: 0.7650 - mse: 0.1618 - val_loss: 0.6314 - val_acc: 0.7500 - val_mse: 0.1876\n",
      "Epoch 171/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5282 - acc: 0.6975 - mse: 0.1902\n",
      "Epoch 00171: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 302us/sample - loss: 0.5028 - acc: 0.7688 - mse: 0.1610 - val_loss: 0.6317 - val_acc: 0.7500 - val_mse: 0.1879\n",
      "Epoch 172/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5100 - acc: 0.7433 - mse: 0.1730\n",
      "Epoch 00172: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.5003 - acc: 0.7700 - mse: 0.1598 - val_loss: 0.6330 - val_acc: 0.7500 - val_mse: 0.1885\n",
      "Epoch 173/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5105 - acc: 0.7450 - mse: 0.1731\n",
      "Epoch 00173: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 366us/sample - loss: 0.4985 - acc: 0.7750 - mse: 0.1590 - val_loss: 0.6360 - val_acc: 0.7450 - val_mse: 0.1896\n",
      "Epoch 174/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5112 - acc: 0.7500 - mse: 0.1733\n",
      "Epoch 00174: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 336us/sample - loss: 0.4976 - acc: 0.7788 - mse: 0.1585 - val_loss: 0.6398 - val_acc: 0.7400 - val_mse: 0.1908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5107 - acc: 0.7483 - mse: 0.1730\n",
      "Epoch 00175: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 346us/sample - loss: 0.4971 - acc: 0.7800 - mse: 0.1582 - val_loss: 0.6423 - val_acc: 0.7400 - val_mse: 0.1918\n",
      "Epoch 176/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5087 - acc: 0.7483 - mse: 0.1724\n",
      "Epoch 00176: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 327us/sample - loss: 0.4940 - acc: 0.7800 - mse: 0.1570 - val_loss: 0.6440 - val_acc: 0.7400 - val_mse: 0.1928\n",
      "Epoch 177/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5091 - acc: 0.7500 - mse: 0.1724\n",
      "Epoch 00177: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 307us/sample - loss: 0.4953 - acc: 0.7812 - mse: 0.1573 - val_loss: 0.6471 - val_acc: 0.7450 - val_mse: 0.1927\n",
      "Epoch 178/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5052 - acc: 0.7517 - mse: 0.1704\n",
      "Epoch 00178: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 280us/sample - loss: 0.4963 - acc: 0.7812 - mse: 0.1573 - val_loss: 0.6513 - val_acc: 0.7500 - val_mse: 0.1931\n",
      "Epoch 179/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4953 - acc: 0.7550 - mse: 0.1665\n",
      "Epoch 00179: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.4911 - acc: 0.7812 - mse: 0.1553 - val_loss: 0.6466 - val_acc: 0.7500 - val_mse: 0.1926\n",
      "Epoch 180/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4886 - acc: 0.7650 - mse: 0.1640\n",
      "Epoch 00180: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 316us/sample - loss: 0.4860 - acc: 0.7875 - mse: 0.1534 - val_loss: 0.6406 - val_acc: 0.7450 - val_mse: 0.1924\n",
      "Epoch 181/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4859 - acc: 0.7667 - mse: 0.1628\n",
      "Epoch 00181: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 406us/sample - loss: 0.4824 - acc: 0.7900 - mse: 0.1518 - val_loss: 0.6403 - val_acc: 0.7500 - val_mse: 0.1932\n",
      "Epoch 182/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4868 - acc: 0.7667 - mse: 0.1629\n",
      "Epoch 00182: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 389us/sample - loss: 0.4714 - acc: 0.7925 - mse: 0.1516 - val_loss: 0.5697 - val_acc: 0.7450 - val_mse: 0.1899\n",
      "Epoch 183/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4857 - acc: 0.7633 - mse: 0.1623\n",
      "Epoch 00183: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 0.4659 - acc: 0.7875 - mse: 0.1514 - val_loss: 0.5448 - val_acc: 0.7400 - val_mse: 0.1837\n",
      "Epoch 184/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4930 - acc: 0.7700 - mse: 0.1617\n",
      "Epoch 00184: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 353us/sample - loss: 0.4732 - acc: 0.7900 - mse: 0.1519 - val_loss: 0.5399 - val_acc: 0.7350 - val_mse: 0.1813\n",
      "Epoch 185/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5735 - acc: 0.7100 - mse: 0.1954\n",
      "Epoch 00185: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 365us/sample - loss: 0.7019 - acc: 0.6275 - mse: 0.2474 - val_loss: 0.7147 - val_acc: 0.5900 - val_mse: 0.2576\n",
      "Epoch 186/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6222 - acc: 0.6333 - mse: 0.2218\n",
      "Epoch 00186: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 350us/sample - loss: 0.7620 - acc: 0.4850 - mse: 0.2844 - val_loss: 0.6835 - val_acc: 0.5050 - val_mse: 0.2488\n",
      "Epoch 187/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5222 - acc: 0.6850 - mse: 0.1737\n",
      "Epoch 00187: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 356us/sample - loss: 0.5784 - acc: 0.6000 - mse: 0.1993 - val_loss: 0.6070 - val_acc: 0.6550 - val_mse: 0.2104\n",
      "Epoch 188/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5517 - acc: 0.7650 - mse: 0.1833\n",
      "Epoch 00188: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 269us/sample - loss: 0.5379 - acc: 0.8087 - mse: 0.1761 - val_loss: 0.5940 - val_acc: 0.7050 - val_mse: 0.2037\n",
      "Epoch 189/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6167 - acc: 0.6600 - mse: 0.2143\n",
      "Epoch 00189: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 309us/sample - loss: 0.5562 - acc: 0.7375 - mse: 0.1859 - val_loss: 0.6003 - val_acc: 0.6700 - val_mse: 0.2070\n",
      "Epoch 190/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6597 - acc: 0.6050 - mse: 0.2340\n",
      "Epoch 00190: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 390us/sample - loss: 0.5790 - acc: 0.6950 - mse: 0.1969 - val_loss: 0.6003 - val_acc: 0.6750 - val_mse: 0.2075\n",
      "Epoch 191/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6558 - acc: 0.6117 - mse: 0.2324\n",
      "Epoch 00191: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 376us/sample - loss: 0.5809 - acc: 0.6988 - mse: 0.1978 - val_loss: 0.5976 - val_acc: 0.6750 - val_mse: 0.2071\n",
      "Epoch 192/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.6209 - acc: 0.6350 - mse: 0.2184\n",
      "Epoch 00192: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 356us/sample - loss: 0.5683 - acc: 0.7050 - mse: 0.1930 - val_loss: 0.6086 - val_acc: 0.6850 - val_mse: 0.2106\n",
      "Epoch 193/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5854 - acc: 0.6600 - mse: 0.2047\n",
      "Epoch 00193: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 310us/sample - loss: 0.5570 - acc: 0.7188 - mse: 0.1886 - val_loss: 0.6389 - val_acc: 0.6800 - val_mse: 0.2158\n",
      "Epoch 194/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5614 - acc: 0.6650 - mse: 0.1954\n",
      "Epoch 00194: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 362us/sample - loss: 0.5577 - acc: 0.7212 - mse: 0.1836 - val_loss: 0.6791 - val_acc: 0.6900 - val_mse: 0.2161\n",
      "Epoch 195/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5332 - acc: 0.6025 - mse: 0.1951\n",
      "Epoch 00195: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 355us/sample - loss: 0.5438 - acc: 0.7362 - mse: 0.1780 - val_loss: 0.6453 - val_acc: 0.7050 - val_mse: 0.2117\n",
      "Epoch 196/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5448 - acc: 0.6833 - mse: 0.1892\n",
      "Epoch 00196: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 362us/sample - loss: 0.5346 - acc: 0.7412 - mse: 0.1744 - val_loss: 0.6195 - val_acc: 0.7000 - val_mse: 0.2069\n",
      "Epoch 197/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5711 - acc: 0.5900 - mse: 0.2095\n",
      "Epoch 00197: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 0.5216 - acc: 0.7500 - mse: 0.1737 - val_loss: 0.5984 - val_acc: 0.7100 - val_mse: 0.2025\n",
      "Epoch 198/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5535 - acc: 0.6850 - mse: 0.1923\n",
      "Epoch 00198: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 350us/sample - loss: 0.5175 - acc: 0.7475 - mse: 0.1731 - val_loss: 0.5812 - val_acc: 0.7100 - val_mse: 0.1987\n",
      "Epoch 199/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5651 - acc: 0.6867 - mse: 0.1958\n",
      "Epoch 00199: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 348us/sample - loss: 0.5257 - acc: 0.7513 - mse: 0.1756 - val_loss: 0.5791 - val_acc: 0.7050 - val_mse: 0.1978\n",
      "Epoch 200/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5636 - acc: 0.6933 - mse: 0.1949\n",
      "Epoch 00200: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 311us/sample - loss: 0.5266 - acc: 0.7538 - mse: 0.1757 - val_loss: 0.5836 - val_acc: 0.7100 - val_mse: 0.1986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5539 - acc: 0.6983 - mse: 0.1910\n",
      "Epoch 00201: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 318us/sample - loss: 0.5226 - acc: 0.7575 - mse: 0.1740 - val_loss: 0.5964 - val_acc: 0.7150 - val_mse: 0.2008\n",
      "Epoch 202/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5430 - acc: 0.7033 - mse: 0.1869\n",
      "Epoch 00202: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 361us/sample - loss: 0.5183 - acc: 0.7600 - mse: 0.1715 - val_loss: 0.6062 - val_acc: 0.7100 - val_mse: 0.2022\n",
      "Epoch 203/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5337 - acc: 0.7117 - mse: 0.1838\n",
      "Epoch 00203: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 341us/sample - loss: 0.5068 - acc: 0.7663 - mse: 0.1681 - val_loss: 0.5937 - val_acc: 0.7200 - val_mse: 0.1995\n",
      "Epoch 204/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5350 - acc: 0.7133 - mse: 0.1840\n",
      "Epoch 00204: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 335us/sample - loss: 0.5046 - acc: 0.7700 - mse: 0.1671 - val_loss: 0.5883 - val_acc: 0.7350 - val_mse: 0.1978\n",
      "Epoch 205/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5362 - acc: 0.7133 - mse: 0.1842\n",
      "Epoch 00205: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 295us/sample - loss: 0.5038 - acc: 0.7713 - mse: 0.1665 - val_loss: 0.5889 - val_acc: 0.7300 - val_mse: 0.1973\n",
      "Epoch 206/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5354 - acc: 0.7100 - mse: 0.1838\n",
      "Epoch 00206: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 0.5031 - acc: 0.7688 - mse: 0.1660 - val_loss: 0.5930 - val_acc: 0.7300 - val_mse: 0.1975\n",
      "Epoch 207/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5612 - acc: 0.6225 - mse: 0.2049\n",
      "Epoch 00207: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 361us/sample - loss: 0.5021 - acc: 0.7725 - mse: 0.1655 - val_loss: 0.5967 - val_acc: 0.7300 - val_mse: 0.1975\n",
      "Epoch 208/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5302 - acc: 0.7150 - mse: 0.1819\n",
      "Epoch 00208: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 413us/sample - loss: 0.4998 - acc: 0.7725 - mse: 0.1646 - val_loss: 0.5962 - val_acc: 0.7300 - val_mse: 0.1972\n",
      "Epoch 209/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5279 - acc: 0.7250 - mse: 0.1810\n",
      "Epoch 00209: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 353us/sample - loss: 0.4974 - acc: 0.7800 - mse: 0.1638 - val_loss: 0.5934 - val_acc: 0.7300 - val_mse: 0.1969\n",
      "Epoch 210/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5268 - acc: 0.7250 - mse: 0.1804\n",
      "Epoch 00210: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 370us/sample - loss: 0.4961 - acc: 0.7800 - mse: 0.1633 - val_loss: 0.5919 - val_acc: 0.7350 - val_mse: 0.1969\n",
      "Epoch 211/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5259 - acc: 0.7233 - mse: 0.1799\n",
      "Epoch 00211: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 421us/sample - loss: 0.4952 - acc: 0.7775 - mse: 0.1628 - val_loss: 0.5929 - val_acc: 0.7350 - val_mse: 0.1973\n",
      "Epoch 212/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5242 - acc: 0.7267 - mse: 0.1791\n",
      "Epoch 00212: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 381us/sample - loss: 0.4939 - acc: 0.7788 - mse: 0.1621 - val_loss: 0.5959 - val_acc: 0.7250 - val_mse: 0.1983\n",
      "Epoch 213/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5221 - acc: 0.7283 - mse: 0.1783\n",
      "Epoch 00213: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 404us/sample - loss: 0.4924 - acc: 0.7800 - mse: 0.1615 - val_loss: 0.5987 - val_acc: 0.7250 - val_mse: 0.1992\n",
      "Epoch 214/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5205 - acc: 0.7300 - mse: 0.1776\n",
      "Epoch 00214: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 0.4912 - acc: 0.7800 - mse: 0.1610 - val_loss: 0.5993 - val_acc: 0.7150 - val_mse: 0.1994\n",
      "Epoch 215/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5397 - acc: 0.6475 - mse: 0.1964\n",
      "Epoch 00215: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 304us/sample - loss: 0.4900 - acc: 0.7800 - mse: 0.1605 - val_loss: 0.5984 - val_acc: 0.7150 - val_mse: 0.1993\n",
      "Epoch 216/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5382 - acc: 0.6550 - mse: 0.1955\n",
      "Epoch 00216: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 321us/sample - loss: 0.4892 - acc: 0.7850 - mse: 0.1601 - val_loss: 0.5973 - val_acc: 0.7200 - val_mse: 0.1990\n",
      "Epoch 217/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5167 - acc: 0.7433 - mse: 0.1758\n",
      "Epoch 00217: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 330us/sample - loss: 0.4887 - acc: 0.7900 - mse: 0.1599 - val_loss: 0.5965 - val_acc: 0.7150 - val_mse: 0.1987\n",
      "Epoch 218/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5152 - acc: 0.7450 - mse: 0.1751\n",
      "Epoch 00218: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 339us/sample - loss: 0.4880 - acc: 0.7900 - mse: 0.1596 - val_loss: 0.5957 - val_acc: 0.7150 - val_mse: 0.1984\n",
      "Epoch 219/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5136 - acc: 0.7433 - mse: 0.1743\n",
      "Epoch 00219: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.4871 - acc: 0.7875 - mse: 0.1592 - val_loss: 0.5949 - val_acc: 0.7150 - val_mse: 0.1981\n",
      "Epoch 220/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5122 - acc: 0.7433 - mse: 0.1737\n",
      "Epoch 00220: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 337us/sample - loss: 0.4861 - acc: 0.7875 - mse: 0.1588 - val_loss: 0.5940 - val_acc: 0.7150 - val_mse: 0.1978\n",
      "Epoch 221/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5113 - acc: 0.7450 - mse: 0.1733\n",
      "Epoch 00221: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 380us/sample - loss: 0.4852 - acc: 0.7887 - mse: 0.1584 - val_loss: 0.5932 - val_acc: 0.7150 - val_mse: 0.1975\n",
      "Epoch 222/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5108 - acc: 0.7450 - mse: 0.1730\n",
      "Epoch 00222: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 314us/sample - loss: 0.4845 - acc: 0.7875 - mse: 0.1581 - val_loss: 0.5926 - val_acc: 0.7150 - val_mse: 0.1973\n",
      "Epoch 223/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5104 - acc: 0.7467 - mse: 0.1728\n",
      "Epoch 00223: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 383us/sample - loss: 0.4839 - acc: 0.7887 - mse: 0.1578 - val_loss: 0.5921 - val_acc: 0.7200 - val_mse: 0.1970\n",
      "Epoch 224/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5099 - acc: 0.7500 - mse: 0.1726\n",
      "Epoch 00224: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 398us/sample - loss: 0.4833 - acc: 0.7912 - mse: 0.1576 - val_loss: 0.5915 - val_acc: 0.7200 - val_mse: 0.1968\n",
      "Epoch 225/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5095 - acc: 0.7500 - mse: 0.1724\n",
      "Epoch 00225: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 328us/sample - loss: 0.4827 - acc: 0.7912 - mse: 0.1574 - val_loss: 0.5908 - val_acc: 0.7200 - val_mse: 0.1966\n",
      "Epoch 226/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5090 - acc: 0.7517 - mse: 0.1722\n",
      "Epoch 00226: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.4822 - acc: 0.7937 - mse: 0.1572 - val_loss: 0.5902 - val_acc: 0.7150 - val_mse: 0.1964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5085 - acc: 0.7517 - mse: 0.1720\n",
      "Epoch 00227: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 0.4816 - acc: 0.7950 - mse: 0.1570 - val_loss: 0.5898 - val_acc: 0.7150 - val_mse: 0.1962\n",
      "Epoch 228/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5081 - acc: 0.7517 - mse: 0.1718\n",
      "Epoch 00228: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 391us/sample - loss: 0.4811 - acc: 0.7937 - mse: 0.1568 - val_loss: 0.5895 - val_acc: 0.7150 - val_mse: 0.1961\n",
      "Epoch 229/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5076 - acc: 0.7517 - mse: 0.1716\n",
      "Epoch 00229: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 342us/sample - loss: 0.4805 - acc: 0.7937 - mse: 0.1565 - val_loss: 0.5894 - val_acc: 0.7250 - val_mse: 0.1960\n",
      "Epoch 230/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5072 - acc: 0.7517 - mse: 0.1714\n",
      "Epoch 00230: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 390us/sample - loss: 0.4799 - acc: 0.7937 - mse: 0.1563 - val_loss: 0.5895 - val_acc: 0.7250 - val_mse: 0.1960\n",
      "Epoch 231/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5273 - acc: 0.6850 - mse: 0.1893\n",
      "Epoch 00231: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 315us/sample - loss: 0.4793 - acc: 0.7937 - mse: 0.1561 - val_loss: 0.5896 - val_acc: 0.7250 - val_mse: 0.1961\n",
      "Epoch 232/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5064 - acc: 0.7550 - mse: 0.1710\n",
      "Epoch 00232: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 0.4785 - acc: 0.7937 - mse: 0.1558 - val_loss: 0.5898 - val_acc: 0.7200 - val_mse: 0.1961\n",
      "Epoch 233/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5061 - acc: 0.7533 - mse: 0.1709\n",
      "Epoch 00233: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 352us/sample - loss: 0.4777 - acc: 0.7925 - mse: 0.1555 - val_loss: 0.5878 - val_acc: 0.7150 - val_mse: 0.1957\n",
      "Epoch 234/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5058 - acc: 0.7500 - mse: 0.1708\n",
      "Epoch 00234: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 352us/sample - loss: 0.4769 - acc: 0.7900 - mse: 0.1551 - val_loss: 0.5840 - val_acc: 0.7100 - val_mse: 0.1941\n",
      "Epoch 235/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5050 - acc: 0.7500 - mse: 0.1705\n",
      "Epoch 00235: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 344us/sample - loss: 0.4761 - acc: 0.7925 - mse: 0.1548 - val_loss: 0.5855 - val_acc: 0.7100 - val_mse: 0.1943\n",
      "Epoch 236/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5045 - acc: 0.7500 - mse: 0.1702\n",
      "Epoch 00236: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 271us/sample - loss: 0.4762 - acc: 0.7912 - mse: 0.1547 - val_loss: 0.5890 - val_acc: 0.7100 - val_mse: 0.1947\n",
      "Epoch 237/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5031 - acc: 0.7533 - mse: 0.1696\n",
      "Epoch 00237: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 312us/sample - loss: 0.4747 - acc: 0.7937 - mse: 0.1541 - val_loss: 0.5872 - val_acc: 0.7150 - val_mse: 0.1948\n",
      "Epoch 238/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5024 - acc: 0.7517 - mse: 0.1698\n",
      "Epoch 00238: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 385us/sample - loss: 0.4721 - acc: 0.7925 - mse: 0.1536 - val_loss: 0.5857 - val_acc: 0.7150 - val_mse: 0.1949\n",
      "Epoch 239/250\n",
      "400/800 [==============>...............] - ETA: 0s - loss: 0.5371 - acc: 0.6775 - mse: 0.1933\n",
      "Epoch 00239: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 305us/sample - loss: 0.4754 - acc: 0.7887 - mse: 0.1546 - val_loss: 0.5913 - val_acc: 0.7150 - val_mse: 0.1956\n",
      "Epoch 240/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.5030 - acc: 0.7483 - mse: 0.1695\n",
      "Epoch 00240: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 329us/sample - loss: 0.4752 - acc: 0.7887 - mse: 0.1541 - val_loss: 0.5982 - val_acc: 0.7100 - val_mse: 0.1959\n",
      "Epoch 241/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4968 - acc: 0.7583 - mse: 0.1670\n",
      "Epoch 00241: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 370us/sample - loss: 0.4732 - acc: 0.7925 - mse: 0.1533 - val_loss: 0.5960 - val_acc: 0.7150 - val_mse: 0.1951\n",
      "Epoch 242/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4910 - acc: 0.7600 - mse: 0.1652\n",
      "Epoch 00242: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 357us/sample - loss: 0.4677 - acc: 0.7937 - mse: 0.1517 - val_loss: 0.5886 - val_acc: 0.7100 - val_mse: 0.1947\n",
      "Epoch 243/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4907 - acc: 0.7600 - mse: 0.1649\n",
      "Epoch 00243: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 370us/sample - loss: 0.4662 - acc: 0.7950 - mse: 0.1511 - val_loss: 0.5856 - val_acc: 0.7100 - val_mse: 0.1945\n",
      "Epoch 244/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4919 - acc: 0.7583 - mse: 0.1650\n",
      "Epoch 00244: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 419us/sample - loss: 0.4673 - acc: 0.7937 - mse: 0.1513 - val_loss: 0.5878 - val_acc: 0.7100 - val_mse: 0.1945\n",
      "Epoch 245/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4904 - acc: 0.7633 - mse: 0.1643\n",
      "Epoch 00245: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 414us/sample - loss: 0.4672 - acc: 0.7962 - mse: 0.1512 - val_loss: 0.5919 - val_acc: 0.7100 - val_mse: 0.1945\n",
      "Epoch 246/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4859 - acc: 0.7667 - mse: 0.1630\n",
      "Epoch 00246: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 332us/sample - loss: 0.4617 - acc: 0.8000 - mse: 0.1493 - val_loss: 0.5928 - val_acc: 0.7100 - val_mse: 0.1957\n",
      "Epoch 247/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4901 - acc: 0.7567 - mse: 0.1649\n",
      "Epoch 00247: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 352us/sample - loss: 0.4624 - acc: 0.7950 - mse: 0.1497 - val_loss: 0.5955 - val_acc: 0.7100 - val_mse: 0.1965\n",
      "Epoch 248/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4943 - acc: 0.7550 - mse: 0.1665\n",
      "Epoch 00248: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 313us/sample - loss: 0.4673 - acc: 0.7912 - mse: 0.1514 - val_loss: 0.6084 - val_acc: 0.7100 - val_mse: 0.1973\n",
      "Epoch 249/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4871 - acc: 0.7633 - mse: 0.1634\n",
      "Epoch 00249: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 264us/sample - loss: 0.4686 - acc: 0.7950 - mse: 0.1517 - val_loss: 0.6104 - val_acc: 0.7250 - val_mse: 0.1957\n",
      "Epoch 250/250\n",
      "600/800 [=====================>........] - ETA: 0s - loss: 0.4762 - acc: 0.7667 - mse: 0.1599\n",
      "Epoch 00250: val_loss did not improve from 0.39986\n",
      "800/800 [==============================] - 0s 347us/sample - loss: 0.4610 - acc: 0.7962 - mse: 0.1498 - val_loss: 0.5966 - val_acc: 0.7200 - val_mse: 0.1945\n",
      "Score on validation set\n",
      "200/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 195us/sample - loss: 0.5966 - acc: 0.7200 - mse: 0.1945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5965502858161926, 0.72, 0.19445768]\n",
      "Score on test set\n",
      "510/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 966us/sample - loss: 0.4388 - acc: 0.7490 - mse: 0.1644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4901902570443995, 0.7490196, 0.16440414]\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 200\n",
    "model_checkpoint = ModelCheckpoint('QuantLSTM.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "csv_logger = CSVLogger('QuantLSTM.log')\n",
    "model.fit(X_TRAIN, Y_TRAIN, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_VALIDATE, Y_VALIDATE), verbose=1, shuffle=False,callbacks=[csv_logger,model_checkpoint])\n",
    "print('Score on validation set')\n",
    "score = model.evaluate(X_VALIDATE, Y_VALIDATE, batch_size=BATCH_SIZE)\n",
    "print(score)\n",
    "print('Score on test set')\n",
    "score = model.evaluate(X_TEST,Y_TEST, batch_size=BATCH_SIZE)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>5.003249</td>\n",
       "      <td>0.510030</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.797389</td>\n",
       "      <td>0.470335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>3.653230</td>\n",
       "      <td>0.465616</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.316037</td>\n",
       "      <td>0.449449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>2.786362</td>\n",
       "      <td>0.438561</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.809532</td>\n",
       "      <td>0.432784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.50125</td>\n",
       "      <td>2.168194</td>\n",
       "      <td>0.417244</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.480549</td>\n",
       "      <td>0.422248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.50125</td>\n",
       "      <td>1.837375</td>\n",
       "      <td>0.402633</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.323817</td>\n",
       "      <td>0.414765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch      acc      loss       mse  val_acc  val_loss   val_mse\n",
       "0      0  0.50000  5.003249  0.510030      0.5  3.797389  0.470335\n",
       "1      1  0.50000  3.653230  0.465616      0.5  3.316037  0.449449\n",
       "2      2  0.50000  2.786362  0.438561      0.5  2.809532  0.432784\n",
       "3      3  0.50125  2.168194  0.417244      0.5  2.480549  0.422248\n",
       "4      4  0.50125  1.837375  0.402633      0.5  2.323817  0.414765"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('QuantLSTM.log',sep=',')\n",
    "print(np.shape(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,EPOCHS+1,1)\n",
    "yt = df['loss'].values\n",
    "yv = df['val_loss'].values\n",
    "plt.plot(x,yt,label='Training Loss')\n",
    "plt.plot(x,yv,label='Validation Loss')\n",
    "plt.legend()\n",
    "#plt.ylim(0.2,0.8)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Binary Cross Entropy Loss')\n",
    "plt.savefig('TrainingLogLSTM.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,EPOCHS+1,1)\n",
    "yt = df['mse'].values\n",
    "yv = df['val_mse'].values\n",
    "plt.plot(x,yt,label='Training Loss')\n",
    "plt.plot(x,yv,label='Validation Loss')\n",
    "plt.legend()\n",
    "#plt.ylim(0.2,0.8)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MeanSquareError')\n",
    "plt.savefig('TrainingLogMSE_LSTM.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel = ks.models.load_model('QuantLSTM.h5')\n",
    "Y_TEST_PRED = loadedModel.predict(X_TEST).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(setName,y_true,y_prob,t):\n",
    "    print('-------------------',setName,'--------------------------')\n",
    "    X = len(y_true)\n",
    "    T = sum(y_true)\n",
    "    F = X-T\n",
    "    print('Total points : ' + str(X))\n",
    "    print('Positive points : ' + str(T))\n",
    "    print('Negative points : ' + str(F))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    auc_roc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    precision, recall, thresholdsPR = precision_recall_curve(y_true, y_prob)\n",
    "    aucPR = auc(recall, precision)\n",
    "\n",
    "    print('AUC ROC:', auc_roc)\n",
    "    print('AUC PR: ', aucPR)\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.title('ROC')\n",
    "    plt.show()\n",
    "    plt.plot(recall,precision)\n",
    "    plt.title('Precision Recall')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    dist_corner = (1-tpr)*(1-tpr) + fpr*fpr\n",
    "    ind = np.argmin(dist_corner)\n",
    "    print('Best threshold according to corner rule and its index: ',thresholds[ind],\"   \",ind)\n",
    "    sens = tpr[ind]\n",
    "    spec = 1 - fpr[ind]\n",
    "    print('Sensitivity: ', sens)\n",
    "    print('Specificity : ', spec)\n",
    "\n",
    "    print('ON  A PARTICULAR THRESHOLD')\n",
    "    print('This threshold being ', t)\n",
    "    dist = abs(t-thresholds)\n",
    "    ind = list(dist).index(min(list(dist)))\n",
    "    print('Best threshold based on point closest to t: ' + str(thresholds[ind]))\n",
    "    sens = tpr[ind]\n",
    "    spec = 1-fpr[ind]\n",
    "    acc = (sens*T + spec*F)/X\n",
    "    print('Sensitivity at threshold : ' + str(sens))\n",
    "    print('Specificiy at threshold : ' + str(spec))\n",
    "    print('Accuracy at threshold : ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- LSTM Classification Metrics on Test Set --------------------------\n",
      "Total points : 510\n",
      "Positive points : 252.0\n",
      "Negative points : 258.0\n",
      "AUC ROC: 0.8937030884705304\n",
      "AUC PR:  0.9142938471426287\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASM0lEQVR4nO3df6zld13n8eer7VajdMAyYzLMtMygQ/QCpiU3HapmrQHWtoaOGMTWEH+kUvxR/QOzSQW3aN0lWdxdjEl3ZaKkQjK0RQMzmsEqWEQJHXsJQ0sHq2Mp02mrvUC3VVkoje/945yLhzvnzv3emfPjns95PpKbnO/3+7nnvD9z733108/3xydVhSRp9p0z7QIkSaNhoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiaC0keTvL/kvxLkn9McluS5wwc/94kf5Hkn5M8leSPkyyseo8tSX47yYn++xzvb2+dfI+kUxnomievqarnAJcAlwK/CpDkcuDPgIPAC4DdwKeBjyd5Ub/N+cBHgJcAVwJbgO8FvghcNtluSMPFO0U1D5I8DPxsVX24v/0O4CVV9cNJ/gq4v6p+YdX3fAhYrqqfTPKzwH8DvqOq/mXC5UudOELX3EmyE7gKOJ7kW+iNtN8/pOmdwKv7r18F/Klhrs3MQNc8+WCSfwYeAZ4A3gZcSO/v4PEh7R8HVubHn79GG2nTMNA1T36kqi4ArgC+i15YPwn8G7B9SPvtwBf6r7+4Rhtp0zDQNXeq6i+B24D/UVX/CnwC+LEhTV9P70QowIeBH0ryrRMpUjoDBrrm1W8Dr05yCXAT8FNJfjnJBUm+Lcl/BS4HfqPf/r30pmr+KMl3JTknyfOTvCXJ1dPpgvSNDHTNpapaBt4D/Jeq+mvgh4AfpTdP/nl6lzV+f1X9fb/9V+mdGP1b4M+Bp4G/oTdtc2TiHZCG8LJFSWqEI3RJaoSBLkmNMNAlqREGuiQ14rxpffDWrVtr165d0/p4SZpJn/zkJ79QVduGHZtaoO/atYulpaVpfbwkzaQkn1/rmFMuktQIA12SGmGgS1IjDHRJaoSBLkmNWDfQk7w7yRNJPrPG8ST5nf6Cufclefnoy5QkrafLCP02eoviruUqYE//6wbg/5x9WZKkjVr3OvSq+liSXadpsg94T/Ue23hPkucl2V5VLtclae4dOHKCg0cf/YZ9Cy/Ywtte85KRf9Yo5tB30Hvw/4qT/X2nSHJDkqUkS8vLyyP4aEnavA4cOcFbPnA/Rz73pYl83ijuFM2QfUMfsl5V+4H9AIuLiz6IXVLTVkbmb3/ty/iJvReP/fNGEegngYsGtncCj43gfSVpLIZNg4zDscefZu/uCycS5jCaQD8E3JjkdmAv8JTz55I2g7WCe2UKZO/uC8f6+Qvbt7DvkqEz0GOxbqAneR9wBbA1yUngbcB/AKiq3wUOA1cDx4EvAz8zrmIlaSMOHn2UY48/zcL2Ld+wf+/uC9l3yY6JjZwnpctVLtetc7yAXxxZRZJ0FgZH5SthfsebLp9yVZMxtcfnStI45rIHp1MmPeUxbQa6pIlbCfJxzGW3Op3ShYEuaayGjcIHg3xew3ccDHRJI7U6wIeNwg3y8TDQpUZN6lrr1VYHuOE9OQa61JDBEJ/UtdarGeDTY6BLM2q9uWmDdf4Y6NImsdEpEuemtZqBLo3JKAL6dAxvrWagS2dpVM8LMaB1tgx06SzN2/NCtHkZ6Jo7o76cb96eF6LNy0BXEzYS0qO+nG/enheizctA19SNYsS8kZB2KkStMtC1IeN+Ot6ZMqQlA11rmORKL4axNBoGuoBuD1Ra2TZ8pc3JQJ8jp5su8YFK0uwz0GfQmc5jn266xACXZp+BPmMOHDnBWz5wP7DxeWxDW2qbgb7JrTW3/fbXvsxglvQNDPQOprVQADi3Lak7A/00xrmQbVcGuKSuDPTTWHnokqEqaRYY6OvwoUuSZoWB3jdsnnzYI1ElabM6Z9oFbBYr0yuDfIqepFky9yP0lZG5z7SWNOvmfoQ+GOaOxiXNsrkfoYMnPiW1Ye5H6JLUik6BnuTKJA8mOZ7kpiHHL05yd5JPJbkvydWjL1WSdDrrBnqSc4FbgauABeC6JAurmv0acGdVXQpcC/zvURcqSTq9LiP0y4DjVfVQVT0D3A7sW9WmgJULtp8LPDa6EiVJXXQJ9B3AIwPbJ/v7Bv068IYkJ4HDwC8Ne6MkNyRZSrK0vLx8BuVKktbSJdAzZF+t2r4OuK2qdgJXA+9Ncsp7V9X+qlqsqsVt27ZtvNoRO3DkxNcfvCVJs65LoJ8ELhrY3smpUyrXA3cCVNUngG8Gto6iwHFaudXf688ltaBLoN8L7EmyO8n59E56HlrV5gTwSoAk300v0GdiTmXv7gt9iqKkJqwb6FX1LHAjcBfwWXpXszyQ5JYk1/Sb/QrwxiSfBt4H/HRVrZ6WkSSNUac7RavqML2TnYP7bh54fQz4vtGWJknaCO8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasTcBrqrFUlqzdwGuqsVSWrNXAb6yujc1YoktWQuA93RuaQWzWWgg2uJSmrP3AW6J0MltWruAt3pFkmtmrtAB6dbJLVpLgNdklpkoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1Ijzpl3ApBw4coKDRx/l2ONPs7B9y7TLkaSR6zRCT3JlkgeTHE9y0xptXp/kWJIHkhwYbZlnbzDMve1fUovWHaEnORe4FXg1cBK4N8mhqjo20GYP8KvA91XVk0m+fVwFn42F7Vu4402XT7sMSRqLLiP0y4DjVfVQVT0D3A7sW9XmjcCtVfUkQFU9MdoyJUnr6RLoO4BHBrZP9vcNejHw4iQfT3JPkiuHvVGSG5IsJVlaXl4+s4olSUN1CfQM2Verts8D9gBXANcBv5fkead8U9X+qlqsqsVt27ZttNYz5jPQJc2DLoF+ErhoYHsn8NiQNger6mtV9TngQXoBvyn4DHRJ86BLoN8L7EmyO8n5wLXAoVVtPgj8IECSrfSmYB4aZaFn4sCRE/z4uz7hgtCS5sK6gV5VzwI3AncBnwXurKoHktyS5Jp+s7uALyY5BtwN/Oeq+uK4iu7q4NFHvx7mjs4lta7TjUVVdRg4vGrfzQOvC3hz/2tT2bv7Qi9VlDQXvPVfkhphoEtSIwx0SWqEgS5JjWg20L2ZSNK8aTbQvZlI0rxpNtABbyaSNFeaDnRJmidNBrrz55LmUZOB7vy5pHnUZKCD8+eS5k9Ti0S7ELSkedbUCN2FoCXNs6ZG6OBC0JLmV1MjdEmaZ80EupcqSpp3zQS6lypKmnfNBDp4qaKk+dZUoEvSPDPQJakRBrokNcJAl6RGGOiS1AgDXZIa0USge1ORJDUS6N5UJEmNBDp4U5EkNRPokjTvDHRJasTMB7onRCWpZ6YD/cCRE7zlA/cDnhCVpE6BnuTKJA8mOZ7kptO0e12SSrI4uhLXtnJ1y9tf+zJPiEqae+sGepJzgVuBq4AF4LokC0PaXQD8MnBk1EWejle3SFJPlxH6ZcDxqnqoqp4Bbgf2DWn3m8A7gK+MsD5JUkddAn0H8MjA9sn+vq9LcilwUVX9yeneKMkNSZaSLC0vL2+4WEnS2roEeobsq68fTM4B3gn8ynpvVFX7q2qxqha3bdvWvUpJ0rq6BPpJ4KKB7Z3AYwPbFwAvBT6a5GHgFcChSZ0YlST1dAn0e4E9SXYnOR+4Fji0crCqnqqqrVW1q6p2AfcA11TV0lgqliQNtW6gV9WzwI3AXcBngTur6oEktyS5ZtwFSpK6Oa9Lo6o6DBxete/mNdpecfZlSZI2aqbvFJUk/btOI/TN5sCRExw8+ijHHn+ahe1bpl2OJG0KMzlCHwxzn+EiST0zOUIHWNi+hTvedPm0y5CkTWMmR+iSpFMZ6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1YuYC/cCRExz53JemXYYkbTozF+gHjz4KwL5Ldky5EknaXDoFepIrkzyY5HiSm4Ycf3OSY0nuS/KRJC8cfan/bu/uC/mJvReP8yMkaeasG+hJzgVuBa4CFoDrkiysavYpYLGqvgf4Q+Adoy5UknR6XUbolwHHq+qhqnoGuB3YN9igqu6uqi/3N+8Bdo62TEnSeroE+g7gkYHtk/19a7ke+NCwA0luSLKUZGl5ebl7lZKkdXUJ9AzZV0MbJm8AFoHfGna8qvZX1WJVLW7btq17lZKkdZ3Xoc1J4KKB7Z3AY6sbJXkV8FbgB6rqq6MpT5LUVZcR+r3AniS7k5wPXAscGmyQ5FLgXcA1VfXE6MuUJK1n3UCvqmeBG4G7gM8Cd1bVA0luSXJNv9lvAc8B3p/kaJJDa7ydJGlMuky5UFWHgcOr9t088PpVI65LkrRBM3enqCRpOANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNaJToCe5MsmDSY4nuWnI8W9Kckf/+JEku0ZdqCTp9NYN9CTnArcCVwELwHVJFlY1ux54sqq+E3gn8N9HXagk6fS6jNAvA45X1UNV9QxwO7BvVZt9wB/0X/8h8MokGV2ZkqT1nNehzQ7gkYHtk8DetdpU1bNJngKeD3xhsFGSG4AbAC6++OIzKnjhBVvO6PskqXVdAn3YSLvOoA1VtR/YD7C4uHjK8S7e9pqXnMm3SVLzuky5nAQuGtjeCTy2Vpsk5wHPBb40igIlSd10CfR7gT1Jdic5H7gWOLSqzSHgp/qvXwf8RVWd0QhcknRm1p1y6c+J3wjcBZwLvLuqHkhyC7BUVYeA3wfem+Q4vZH5teMsWpJ0qi5z6FTVYeDwqn03D7z+CvBjoy1NkrQR3ikqSY0w0CWpEQa6JDXCQJekRmRaVxcmWQY+f4bfvpVVd6HOAfs8H+zzfDibPr+wqrYNOzC1QD8bSZaqanHadUySfZ4P9nk+jKvPTrlIUiMMdElqxKwG+v5pFzAF9nk+2Of5MJY+z+QcuiTpVLM6QpckrWKgS1IjNnWgz+Pi1B36/OYkx5Lcl+QjSV44jTpHab0+D7R7XZJKMvOXuHXpc5LX93/WDyQ5MOkaR63D7/bFSe5O8qn+7/fV06hzVJK8O8kTST6zxvEk+Z3+v8d9SV5+1h9aVZvyi96jev8BeBFwPvBpYGFVm18Afrf/+lrgjmnXPYE+/yDwLf3XPz8Pfe63uwD4GHAPsDjtuifwc94DfAr4tv72t0+77gn0eT/w8/3XC8DD0677LPv8H4GXA59Z4/jVwIforfj2CuDI2X7mZh6hz+Pi1Ov2uarurqov9zfvobeC1Czr8nMG+E3gHcBXJlncmHTp8xuBW6vqSYCqemLCNY5alz4XsLJo8HM5dWW0mVJVH+P0K7ftA95TPfcAz0uy/Ww+czMH+rDFqXes1aaqngVWFqeeVV36POh6ev+Fn2Xr9jnJpcBFVfUnkyxsjLr8nF8MvDjJx5Pck+TKiVU3Hl36/OvAG5KcpLf+wi9NprSp2ejf+7o6LXAxJSNbnHqGdO5PkjcAi8APjLWi8Tttn5OcA7wT+OlJFTQBXX7O59GbdrmC3v+F/VWSl1bV/x1zbePSpc/XAbdV1f9Mcjm9VdBeWlX/Nv7ypmLk+bWZR+jzuDh1lz6T5FXAW4FrquqrE6ptXNbr8wXAS4GPJnmY3lzjoRk/Mdr1d/tgVX2tqj4HPEgv4GdVlz5fD9wJUFWfAL6Z3kOsWtXp730jNnOgz+Pi1Ov2uT/98C56YT7r86qwTp+r6qmq2lpVu6pqF73zBtdU1dJ0yh2JLr/bH6R3ApwkW+lNwTw00SpHq0ufTwCvBEjy3fQCfXmiVU7WIeAn+1e7vAJ4qqoeP6t3nPaZ4HXOEl8N/B29s+Nv7e+7hd4fNPR+4O8HjgN/A7xo2jVPoM8fBv4JONr/OjTtmsfd51VtP8qMX+XS8ecc4H8Bx4D7gWunXfME+rwAfJzeFTBHgf807ZrPsr/vAx4HvkZvNH498HPAzw38jG/t/3vcP4rfa2/9l6RGbOYpF0nSBhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRH/H5O3ENPcU8X8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold according to corner rule and its index:  0.29113963     44\n",
      "Sensitivity:  0.8571428571428571\n",
      "Specificity :  0.9069767441860466\n",
      "ON  A PARTICULAR THRESHOLD\n",
      "This threshold being  0.5\n",
      "Best threshold based on point closest to t: 0.42625287\n",
      "Sensitivity at threshold : 0.8293650793650794\n",
      "Specificiy at threshold : 0.9302325581395349\n",
      "Accuracy at threshold : 0.8803921568627451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "getMetrics('LSTM Classification Metrics on Test Set',Y_TEST,Y_TEST_PRED,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics2(setName,y_true,y_prob,t):\n",
    "    print('-------------------',setName,'--------------------------')\n",
    "    X = len(y_true)\n",
    "    T = sum(y_true)\n",
    "    F = X-T\n",
    "    print('Total points : ' + str(X))\n",
    "    print('Positive points : ' + str(T))\n",
    "    print('Negative points : ' + str(F))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    auc_roc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    precision, recall, thresholdsPR = precision_recall_curve(y_true, y_prob)\n",
    "    aucPR = auc(recall, precision)\n",
    "\n",
    "    print('AUC ROC:', auc_roc)\n",
    "    print('AUC PR: ', aucPR)\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.title('ROC')\n",
    "    plt.show()\n",
    "    plt.plot(recall,precision)\n",
    "    plt.title('Precision Recall')\n",
    "    plt.show()\n",
    "    \n",
    "    ind = np.argmax(tpr - fpr)\n",
    "    print('Best threshold according to corner rule and its index: ',thresholds[ind],\"   \",ind)\n",
    "    sens = tpr[ind]\n",
    "    spec = 1 - fpr[ind]\n",
    "    print('Sensitivity: ', sens)\n",
    "    print('Specificity : ', spec)\n",
    "\n",
    "    print('ON  A PARTICULAR THRESHOLD')\n",
    "    print('This threshold being ', t)\n",
    "    dist = abs(t-thresholds)\n",
    "    ind = list(dist).index(min(list(dist)))\n",
    "    print('Best threshold based on point closest to t: ' + str(thresholds[ind]))\n",
    "    sens = tpr[ind]\n",
    "    spec = 1-fpr[ind]\n",
    "    acc = (sens*T + spec*F)/X\n",
    "    print('Sensitivity at threshold : ' + str(sens))\n",
    "    print('Specificiy at threshold : ' + str(spec))\n",
    "    print('Accuracy at threshold : ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- LSTM Classification Metrics on Test Set --------------------------\n",
      "Total points : 510\n",
      "Positive points : 252.0\n",
      "Negative points : 258.0\n",
      "AUC ROC: 0.8937030884705304\n",
      "AUC PR:  0.9142938471426287\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold according to corner rule and its index:  0.36829844     38\n",
      "Sensitivity:  0.8373015873015873\n",
      "Specificity :  0.9302325581395349\n",
      "ON  A PARTICULAR THRESHOLD\n",
      "This threshold being  0.5\n",
      "Best threshold based on point closest to t: 0.42625287\n",
      "Sensitivity at threshold : 0.8293650793650794\n",
      "Specificiy at threshold : 0.9302325581395349\n",
      "Accuracy at threshold : 0.8803921568627451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "getMetrics2('LSTM Classification Metrics on Test Set',Y_TEST,Y_TEST_PRED,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
